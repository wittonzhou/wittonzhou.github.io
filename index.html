<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="f95QClvWkWuCmeDiyOR8qqZeVKyONBqEfTEnHD_rpa4" />








  <meta name="baidu-site-verification" content="1l6QQqPWSs" />







  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/wittonzhou-medium.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/wittonzhou-small.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="WittonZhou">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="WittonZhou">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="WittonZhou">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"remove","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>WittonZhou</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">WittonZhou</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">业精于勤荒于嬉，行成于思毁于随</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/Graph-Embedding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Witton Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WittonZhou">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Graph-Embedding/" itemprop="url">Graph Embedding</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-10T21:40:36+08:00">
                2019-07-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Random-Walk"><a href="#Random-Walk" class="headerlink" title="Random Walk"></a>Random Walk</h2><p>先介绍一下Random Walk。<br>Random Walk从图上获得一条随机的路径。Random Walk从一个节点开始，随机沿着一条边正向或者反向寻找到它的邻居，以此类推，直到达到设置的路径长度。  </p>
<h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><p>Embedding 在数学上表示一个映射关系， F: X -&gt; Y， 也就是一个函数。深度学习中，Embedding 特指用一个低维度向量表示一个实体，可以是一个词（Word2Vec），可以是一个物品（Item2Vec），更或者是网络关系中的节点（Graph Embedding）。  </p>
<p>Embedding 拥有很多优点：得到的向量表达维度更低（避免了独热编码带来的稀疏问题），并且表达了实体间内在的关系。  </p>
<p>Embedding主要的三个应用方向：  </p>
<ol>
<li>在深度学习网络中使用 Embedding 层，将高维稀疏特征向量转换成低维稠密特征向量，从而减少后续模型参数量，后续模型可以是深度学习模型，或者传统的机器学习模型；  </li>
<li>作为预训练技术，直接使用别人训练好的 Embedding 向量，与其他特征向量一同输入后续模型进行训练，例如 Word2Vec；  </li>
<li>通过计算用户和物品的 Embedding 相似度，Embedding 可以直接作为推荐系统或计算广告系统的召回层或者召回方法之一，例如 Youtube 推荐系统。</li>
</ol>
<h2 id="Graph-Embedding"><a href="#Graph-Embedding" class="headerlink" title="Graph Embedding"></a>Graph Embedding</h2><p>Graph Embedding 用低维、稠密、实值的向量表示网络中的节点。目前，Graph Embedding 已经是推荐系统、计算广告领域非常热门的做法。</p>
<p>Word2Vec通过序列（sequence）式的样本：句子，来学习单词的真实含义。仿照 Word2Vec 思想而生的 Item2Vec 也通过Item的组合去生成Item的 Embedding，这里Item的组合也是序列式的，我们可以称他们为“Sequence Embedding”。下图为淘宝用户行为数据生成的Item Graph。  </p>
<h1 id=""><a href="#" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-v2-1ecd09021d8549db8a25e011a42a5637_hd.jpg" alt=""></h1><p>可以看出，Graph Embedding能够生成一些“不存在”的物品关系序列。比如B-E-F。</p>
<h2 id="Graph-Embedding早期技术：DeepWalk"><a href="#Graph-Embedding早期技术：DeepWalk" class="headerlink" title="Graph Embedding早期技术：DeepWalk"></a>Graph Embedding早期技术：DeepWalk</h2><p>简单来说，DeepWalk是Random Walk和Skip-gram的组合。下图是DeepWalk的伪代码。  </p>
<h1 id="-1"><a href="#-1" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-v2-d63bbd04c0ca7a98b2fac7eececb472d_hd.jpg" alt=""></h1><p>可以看出，Random Walk负责对图进行采样，获得图中节点与节点的共西现关系，Skip-gram 从关系（也就是采样的序列）中训练节点的 Embedding 向量。</p>
<p>参考链接：<br><a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/7PZ6L1YzPZdiMNK25sUqjg" target="_blank" rel="noopener">https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/7PZ6L1YzPZdiMNK25sUqjg</a>  </p>
<p><a href="https://zhuanlan.zhihu.com/p/68247149" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/68247149</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/优化算法总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Witton Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WittonZhou">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/优化算法总结/" itemprop="url">优化算法总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-11T09:34:08+08:00">
                2018-12-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="梯度下降法的问题"><a href="#梯度下降法的问题" class="headerlink" title="梯度下降法的问题"></a>梯度下降法的问题</h3><p>学习率设定：学习率的设定带来的挑战有三方面。  </p>
<p>首先，选择一个合理的学习率很难，如果学习率过小，则会导致收敛速度慢。如果学习率过大，会阻碍收敛，即出现在极值点附近振荡的现象。  </p>
<p>其次，学习率调整很难，我们一般使用某种事先设定的策略或者在每次迭代中衰减一个较小的阈值。无论哪种调整方法，都需要事先进行固定设置，这边便无法自适应每次学习的数据集特点。  </p>
<p>最后，模型所有的参数每次更新都是使用相同的学习速率。如果数据特征是稀疏的或者每个特征有着不同的取值统计特征与空间，那么便不能在每次更新中每个参数使用相同的学习速率，那些很少出现的特征应该使用一个相对较大的学习速率。</p>
<p>另外，对于非凸目标函数，容易陷入局部极值点或鞍点之中。</p>
<h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>SGD方法的一个缺点是其更新方向完全依赖于当前batch计算出的梯度，因而十分不稳定。Momentum算法借用了物理中的动量概念，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力。</p>
<p>优点：  </p>
<ul>
<li>对方向一致的参数能够加速学习，对梯度改变方向的参数能够减少其更新，因此，momentum能够在相关方向上加速学习，抑制振荡，从而加速收敛。  </li>
</ul>
<p>缺点：  </p>
<ul>
<li>较好的学习率难以获得。</li>
</ul>
<h2 id="自适应学习率算法"><a href="#自适应学习率算法" class="headerlink" title="自适应学习率算法"></a>自适应学习率算法</h2><p>损失通常高度敏感于参数空间中的某些方向，而不敏感于其他。动量算法可以在一定程度上缓解这个问题，但这样做的代价是引入了另一个超参数。  </p>
<h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><p>独立地适应所有模型参数的学习率，缩放每个参数反比与其所有梯度历史平方值总和的平方根。Adagrad算法能够在训练中自动的对learning rate进行调整，对于出现频率较低参数采用较大的α更新；相反，对于出现频率较高的参数采用较小的α更新。因此，Adagrad非常适合处理稀疏数据。 </p>
<p>优点：  </p>
<ul>
<li>它能够为每个参数自适应不同的学习速率，而一般的人工都是设定为0.01。 </li>
</ul>
<p>缺点：  </p>
<ul>
<li><p>在于需要计算参数梯度序列平方和，并且学习速率趋势是不断衰减最终达到一个非常小的值。即，在训练的中后期，分母上梯度平方的累加将会越来越大，从而梯度趋近于0，使得训练提前结束。  </p>
</li>
<li><p>依赖一个全局学习率。</p>
</li>
</ul>
<h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><p>Adagrad会累加之前所有的梯度平方，而RMSprop仅仅是计算对应的平均值，具体来说，是使用指数加权平均来代替历史梯度的平方和，因此可缓解Adagrad算法学习率下降较快的问题。</p>
<p>优点：  </p>
<ul>
<li>RMSprop改进了Adagrad学习速率衰减过快的问题，同时其适用于处理非平稳。</li>
</ul>
<p>缺点：  </p>
<ul>
<li>依然依赖一个全局学习率。</li>
</ul>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>Adam(Adaptive Moment Estimation)是另一种自适应学习率的方法。它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。</p>
<p>优点：  </p>
<ul>
<li>Adam 算法结合了 Momentum 和 RMSprop 梯度下降法，并且是一种极其常用的学习算法，被证明能有效适用于不同神经网络，适用于广泛的结构。</li>
<li>结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/读论文-《Wide-Deep》/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Witton Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WittonZhou">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/读论文-《Wide-Deep》/" itemprop="url">读论文-《Wide&Deep》</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-27T20:16:53+08:00">
                2018-11-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="《Wide-amp-Deep-Learning-for-Recommender-Systems》-2016-Google"><a href="#《Wide-amp-Deep-Learning-for-Recommender-Systems》-2016-Google" class="headerlink" title="《Wide&amp;Deep Learning for Recommender Systems》,2016,Google"></a>《Wide&amp;Deep Learning for Recommender Systems》,2016,Google</h2><h3 id="Wide-amp-Deep介绍"><a href="#Wide-amp-Deep介绍" class="headerlink" title="Wide&amp;Deep介绍"></a>Wide&amp;Deep介绍</h3><p>推荐系统可以被视为搜索排名系统，其中输入query是一组用户和上下文信息，输出的是items的排序列表。给定一个query，推荐的任务就是在数据库中找到相关items，然后基于某些目标（例如点击或购买）对items进行排名。  </p>
<p>与一般搜索排名问题类似，推荐系统中的一个挑战是实现记忆和泛化。记忆可以被宽松地定义为学习items或特征之间的相关频率，在历史数据中探索相关性的可行性。另一方面，泛化是基于相关性的传递，并探索过去从未或很少发生的新特征组合。基于记忆的推荐结果通常更加局限性的，是在用户和items已经有直接关联的活动上。与记忆相比，泛化倾向于改善推荐items的多样性。在这篇paper中，我们主要关注Google Play商店的app推荐问题，但是该方法对推荐系统具有通用性。  </p>
<p>对于工业环境中的大规模在线推荐和排名系统，诸如逻辑回归的广义线性模型被广泛使用，因为它们是简单的，可扩展性好和可解释性强。模型通常使用独热编码训练二值化稀疏特征。例如，如果用户安装了Netflix，则二进制特征“user_installed_app=netflix”的值为1。记忆可以通过稀疏特征进行交叉乘积（叉乘，也叫做外积或向量积）转换获得，例如AND（user_installed_app = netflix，impression_app = pandora），当用户安装了Netflix并且之后展示在Pandora上，那么得到特征的值为1，其余为0。这个交叉特征就展示了特征对之间的相关性和目标label之间的关联。泛化可以通过增加一些粗粒度的特征实现，（例如AND（user_installed_category = video，impression_category = music）），但通常需要手动进行特征工程。交叉积转换的一个限制是它们不能生成未出现在训练数据中的query-item特征对。这里主要是对接下来线性模型需要的特征做了下解释，一个是one-hot，比较稀疏。一个是交叉特征，就是特征之间做笛卡尔积，用于线性模型去寻找显性的非线性。  </p>
<p>基于embedding的模型，例如FM或DNN，可以通过为每个query和item学习出一个低维稠密嵌入向量，来推广到先前未见过的query-item特征对，这显然减少特征工程的负担。然而，当潜在的query-item矩阵稀疏且高秩时，例如具有特定偏好的用户或很少出现的小众items，很难学习query和item的有效低维表示。在这种情况下，大多数query-item对之间不存在交互，但是稠密嵌入将导致对所有query-item对的非零预测，因此可能过度泛化并且推荐不太相关的内容。另一方面，利用交叉积特征的线性模型能用很少的参数记住那些‘exception_rules’。embedding特征，也就是把稀疏数据映射到稠密的低维数据。   </p>
<p>文章中提出了Wide&amp;Deep学习框架，通过联合训练线性模型部分和NN部分，在一个模型中实现记忆和泛化。Wide&amp;Deep模型的结构图如下所示。  </p>
<h1 id=""><a href="#" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E7%BB%93%E6%9E%84%E5%9B%BE.png" alt=""></h1><p>Wide&amp;Deep学习框架，用于联合训练具有嵌入的前馈神经网络和具有特征变换的线性模型，可以用于具有稀疏输入的通用推荐系统。</p>
<h3 id="推荐系统概述"><a href="#推荐系统概述" class="headerlink" title="推荐系统概述"></a>推荐系统概述</h3><p>App的推荐系统如下图所示。当用户访问应用商店时，会生成一个query，其中包括各种用户和上下文功能。推荐系统返回App列表（也称为印象），用户可以在其上执行某些操作，例如点击或购买。这些用户操作以及查询和印象记录在日志中，作为学习器的训练数据。  </p>
<p>由于数据库中有过百万的应用程序，因此对全部app计算score不合理。因此，收到一个query的第一步是检索。检索系统返回一个items的短列表，通常是机器学习模型和人为定义规则的组合。在减少候选池之后，排序系统按照他们的score对所有项目进行排名。分数通常为P（y | x），给定特征x的用户行为y的概率，包括用户特征（例如，国家，语言，人口统计），上下文特征（例如，设备，一天中的小时，day of the week）和印象特征（例如，应用年龄，应用的历史统计数据）。在本文中，重点为使用Wide＆Deep学习框架的排序模型上。  </p>
<h1 id="-1"><a href="#-1" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-12-01%20%E4%B8%8A%E5%8D%889.37.44.png" alt=""></h1><h3 id="Wide-amp-Deep-Learning"><a href="#Wide-amp-Deep-Learning" class="headerlink" title="Wide&amp;Deep Learning"></a>Wide&amp;Deep Learning</h3><p>1.Wide部分  </p>
<p>Wide部分的形式是y = w T x + b的广义线性模型，y是预测，x=[x1,x2,x3…xd]是d维特征的向量，w=[w1,w2,w3…wd]是模型参数，b是偏置项。特征集合包括了原始的输入特征和转化后的特征。其中，最重要的转化就是交叉乘积转换，  </p>
<h1 id="-2"><a href="#-2" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-12-01%20%E4%B8%8A%E5%8D%8810.47.24.png" alt=""></h1><p>cki是一个boolean值变量，当第i个特征是第k个转换ϕk，否则的就是0。对于一个二进制特征，交叉积特征可以简单理解为AND(gender=female, language=en),当且仅当gender=female，language=en时，交叉特征为1，其他都为0。该方法能捕捉出特征间的交互，为模型添加非线性。</p>
<p>2.Deep部分  </p>
<p>Deep模块则是一个前向神经网络，对于类别型特征，原始输入特征其原始输入都是字符串形式的特征，如“language=en”.我们把这些稀疏，高维的类别型特征转换为低维稠密的实值向量，这就是embedding向量。embedding随机初始化，并利用反向传播对其进行更新。将高维的特征换换为embedding特征后，这些低维的embedding向量就被feed到神经网络中，每个隐藏层做如下计算：  </p>
<h1 id="-3"><a href="#-3" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-12-01%20%E4%B8%8A%E5%8D%8810.50.47.png" alt=""></h1><p>其中l是网络的层数，f是激活函数，一般用RELU，al,bl,Wl分别为第l层的激活函数，偏置项，模型权值。<br>也就是说输入的类别型特征是字符串，需要转化下，然后做embedding，模型是一个全连接。</p>
<p>3.Joint Training of Wide&amp;Deep Model  </p>
<p>通过将Wide部分和Deep部分的对数加权输出作为预测值，然后将其feed给一个常规的逻辑损失函数中，用于联合训练。需要注意的是，联合训练和ensemble是有区别的。在集成方法中，模型都是独立训练的，模型之间没有关系，他们的预测输出只在最后才合并。但是，联合训练的话，两个模型是一起训练所有参数。对于模型大小来说，集成方法，因为模型之间独立，所以单个模型的大小需要更大，即需要更多的特征和特征工程。以此起来获得合理的精度。但是联合训练，两个模块只要互相补充对方不足即可，可以同时优化模型的参数。在Wide&amp;Deep中，只需要两个模型，训练简单，可以很快的迭代模型。  </p>
<p>Wide&amp;Deep模型的联合训练通过反向传播将输出值的误差梯度通过mini-batch随机梯度同时传送给Wide和Deep部分。在实验中，我们使用带L1的FTRL算法作为Wide模块的优化器，使用AdaGrad更新Deep模块。  </p>
<p>对于逻辑回归问题，我们模型的预测是：  </p>
<h1 id="-4"><a href="#-4" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-12-01%20%E4%B8%8A%E5%8D%8810.54.15.png" alt=""></h1><p>其中Y是一个二值的类别标签，σ()是sigmoid函数，ϕ(x)表示交叉特征，b是一个bias项，Wwide是Wide模型的权值，Wdeep是应用在最后的隐藏层到输出上的权值。  </p>
<h3 id="系统实现"><a href="#系统实现" class="headerlink" title="系统实现"></a>系统实现</h3><p>1.数据生成<br>在此阶段，一段时间内的用户和应用程序展示数据用于生成训练数据。标签是应用程序获取：如果安装了展示的应用程序，则为1，否则为0。  </p>
<h1 id="-5"><a href="#-5" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-12-01%20%E4%B8%8A%E5%8D%8810.34.19.png" alt=""></h1><p>词汇表，即将分类要素字符串映射到整数ID的表，也在此阶段生成。系统会计算发生超过最少次数的所有字符串要素的ID空间。 通过将特征值x映射到其累积分布函数P（X≤x），将连续的实值特征归一化为[0,1]，将其分成n_q个分位数。对于第i个分位数中的值，归一化值是(i-1)/(n_q-1)。 在数据生成阶段计算分位数边界。  </p>
<p>2.模型训练<br>我们在实验中使用的模型结构如下图所示。在训练期间，我们的输入层接收训练数据和词汇表，并生成稀疏和稠密的特征以及标签。Wide部分包括用户安装的应用和展示应用的交叉乘积。对于模型的Deep部分，为每个分类特征学习32维嵌入向量。我们将所有嵌入与密集特征连接在一起，产生大约1200维的稠密向量。然后将连接的向量输入3个ReLU层，最后输入logistic output单元。  </p>
<h1 id="-6"><a href="#-6" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-12-01%20%E4%B8%8A%E5%8D%8810.36.02.png" alt=""></h1><p>Wide＆Deep模型已接受超过5000亿条数据的训练。每当一组新的训练数据到达时，该模型需要重新训练。然而，每次从头开始重新训练都是计算上昂贵的，并且延迟了从数据到达到服务更新模型的时间。为了应对这一挑战，我们实施了一个热启动系统，该系统使用先前模型中的嵌入和线性模型权重初始化新模型。  </p>
<p>在将模型加载到模型服务器之前，完成模型的干运行以确保它不会导致提供实时交易的问题。我们根据先前的模型验证模型质量作为健全性检查。</p>
<p>3.模型服务<br>模型经过训练和验证后，我们将其加载到模型服务器中。对于每个query，服务器从应用程序检索系统接收一组应用程序候选者，并接收用户特征来为每个应用程序评分。然后，应用程序从最高分数到最低分数排名，我们按此顺序向用户显示应用程序。通过在Wide＆Deep模型上运行前向推理传递来计算得分。  </p>
<p>为了按10毫秒的顺序提供每个请求，我们通过并行运行较小批量使用多线程并行性来优化性能，而不是在单个批量推理步骤中对所有候选应用程序进行评分。  </p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>为了评估Wide&amp;Deep学习在现实世界推荐系统中的有效性，我们进行了实时实验，并在几个方面评估了系统：应用程序获取和服务性能。  </p>
<p>我们在A/B测试框架中进行了3周的实时在线实验。对于对照组，1％的用户被随机选择并呈现由先前版本的排名模型生成的推荐，这是一种高度优化的wide-only逻辑回归模型，具有丰富的交叉积转换。对于实验组，1％的用户会收到由Wide＆Deep模型生成的建议，并使用相同的特征集进行训练。如表1所示，Wide＆Deep模型相对于对照组（统计显着性）将应用商店主登陆页面上的应用获取率提高了3.9％。结果也与另一个1％组进行比较，仅使用具有相同特征和神经网络结构的模型的Deep部分，Wide&amp;Deep在深度模型之上具有+1％的增益。</p>
<p>除了在线实验，还比较了AUC on holdout set。虽然Wide＆Deep的AUC略高，但是Wide&amp;Deep模型对在线交易比线下实验的影响更为明显。一个可能的原因是，固定数据集中的印象和标签是固定的，而在线系统可以通过将泛化与记忆相结合来产生新的探索性建议，并从新的用户响应中学习。  </p>
<h1 id="-7"><a href="#-7" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-12-01%20%E4%B8%8A%E5%8D%8810.58.41.png" alt=""></h1><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><p>将宽线性模型与交叉乘积变换和深层神经网络与密集嵌入相结合的想法受到以前的工作的启发，例如FM，它通过将两个变量之间的相互作用分解为两个低维嵌入向量之间点积（也就是内积）来增加线性模型的推广。在本文中，我们通过神经网络而不是点积来学习嵌入之间的高度非线性交互来扩展模型容量。  </p>
<p>在语言模型中，已提出联合训练循环神经网络（RNN）和具有n-gram特征的最大熵模型，通过学习输入和输出之间的直接权重来显着降低RNN复杂度（例如，隐藏层大小）。在计算机视觉中，深度残差学习已被用于减少训练更深层模型的难度，并通过跳过一个或多个层的快捷连接来提高准确性。神经网络与图形模型的联合训练也已应用于图像中的人体姿态估计。在这项工作中，我们探索了前馈神经网络和线性模型的联合训练，稀疏特征和输出单元之间的直接连接，用于稀疏输入数据的通用推荐和排序问题。  </p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>记忆和泛化对于推荐系统都很重要。宽线性模型可以使用叉乘特征变换有效地记忆稀疏特征交互，而深度神经网络可以通过低维嵌入推广到先前未见过的特征交互。我们介绍了Wide＆Deep学习框架，以结合两种模型的优势。我们在Google Play的推荐系统上制作并评估了该框架，在线实验结果表明，Wide＆Deep模型在广泛和深度模型上的应用程序获取方面取得了显着的进步。  </p>
<h3 id="个人体会"><a href="#个人体会" class="headerlink" title="个人体会"></a>个人体会</h3><p>本文中，Google公司为推荐系统和CTR预测提出了Wide&amp;Deep模型，并在Github上进行了开源。  </p>
<p>在Wide&amp;Deep模型中，Wide部分和Deep部分是同时训练的，而不是单独进行训练的。而在GBDT+LR（Facebook提出的CTR预测方法）模型中，GBDT需要先训练，然后再训练LR，这两部分有依赖关系，不利于模型的迭代。  </p>
<p>总体来说，这篇paper的影响力是很大的，Wide&amp;Deep模型的应用场景也是很广泛的，在一些推荐算法的比赛中也取得了非常好的成绩。但是Wide&amp;Deep模型的缺点就是需要进行人工的特征工程，在2017年华为诺亚方舟实验室和哈工大共同提出的DeepFM改进了这一缺点。Google在2017年还提出了CTR预测方法Deep&amp;Cross，性能效果比Wide&amp;Deep更好一些，但是目前还没有得到广泛的应用。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/读论文《DeepFM》/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Witton Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WittonZhou">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/读论文《DeepFM》/" itemprop="url">读论文《DeepFM》</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-19T21:05:20+08:00">
                2018-11-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h2 id="DeepFM"><a href="#DeepFM" class="headerlink" title="DeepFM"></a>DeepFM</h2><h3 id="DeepFM-A-Factorization-Machine-based-Neural-Network-for-CTR-Prediction-2017"><a href="#DeepFM-A-Factorization-Machine-based-Neural-Network-for-CTR-Prediction-2017" class="headerlink" title="DeepFM: A Factorization-Machine based Neural Network for CTR Prediction, 2017"></a>DeepFM: A Factorization-Machine based Neural Network for CTR Prediction, 2017</h3><p>文章所提出的模型DeepFM结合了因子分解机和深度学习的功能，分解机的功能用于推荐，深度学习的功能用于新的神经网络架构中的特征学习。与Google最新的Wide＆Deep模型相比，DeepFM对其“宽”和“深”部分有共享输入，除了原始功能之外不需要特征工程。  </p>
<p>点击率（CTR）的预测在推荐系统中至关重要，其中任务是估计用户点击推荐项目的概率。 在许多推荐系统中，目标是最大化点击次数，因此返回给用户的项目可以按估计的CTR排序; 而在其他应用场景中，例如在线广告，提高收入也很重要，因此排名策略可以调整为所有候选人的CTR×出价，其中“出价”是系统在用户点击项目时收到的好处。在任何一种情况下，很明显关键在于正确估算点击率。  </p>
<p>CTR预测学习用户点击行为背后的隐式功能交互非常重要。通过我们在主流应用程序市场的研究，我们发现人们经常在用餐时下载用于提供食物的应用程序，这表明应用程序类别和时间戳之间的交互可以用作CTR的信号。再比如，男性青少年喜欢射击游戏和RPG游戏，这意味着应用类别，用户性别和年龄的交互是CTR的另一个信号。通常，用户点击行为背后的特征的这种交互可以是高度复杂的，其中低阶和高阶特征交互都应该发挥重要作用。根据Wide&amp;Deep提出的观点可知，同时考虑低阶和高阶特征相互作用比在单独考虑其中的情况下带来额外的改进。</p>
<p>关键的挑战是有效地建模特征交互。即使对于易于理解的交互，专家似乎也不可能对其进行前所未有的建模，尤其是当特征数量很大时。  </p>
<p>虽然原则上FM可以模拟高阶特征交互，但实际上通常由于高复杂性而仅考虑2阶特征交互。</p>
<p>现有模型偏向于低阶或高阶特征交互，或依赖于特征工程。在文章中，展示了可以推导出一种学习模型，该模型能够以端到端的方式学习所有的特征交互，除了原始特征之外没有任何特征工程。  </p>
<p>DeepFM模拟了FM的低阶特征交互，还模拟了DNN的高阶特征交互。与Wide&amp;Deep不同，DeepFM没有进行任何特征工程。  </p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/读论文《DeepFM》/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/浅析Xgboost原理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Witton Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WittonZhou">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/浅析Xgboost原理/" itemprop="url">浅析Xgboost原理</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-15T22:31:16+08:00">
                2018-11-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="浅析Xgboost原理"><a href="#浅析Xgboost原理" class="headerlink" title="浅析Xgboost原理"></a>浅析Xgboost原理</h2><h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ol>
<li>泰勒公式</li>
<li>最优化方法</li>
<li>从参数空间到函数空间</li>
<li>详解Xgboost</li>
<li>LightGBM</li>
</ol>
<hr>
<p>泰勒公式：</p>
<h1 id=""><a href="#" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E6%B3%B0%E5%8B%92%E5%85%AC%E5%BC%8F.png" alt=""></h1><p>梯度下降法：</p>
<h1 id="-1"><a href="#-1" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.png" alt=""></h1><p>牛顿法：</p>
<h1 id="-2"><a href="#-2" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E7%89%9B%E9%A1%BF%E6%B3%95.png" alt=""></h1><h3 id="从参数空间到函数空间"><a href="#从参数空间到函数空间" class="headerlink" title="从参数空间到函数空间"></a>从参数空间到函数空间</h3><p>GBDT在函数空间中利用梯度下降法进行优化<br>Xgboost在函数空间中用牛顿法进行优化  </p>
<h1 id="-3"><a href="#-3" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-16%20%E4%B8%8B%E5%8D%883.21.41.png" alt=""></h1><p>Xgboost的正则项：  </p>
<h1 id="-4"><a href="#-4" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E6%AD%A3%E5%88%99%E9%A1%B9.png" alt=""></h1><p>上面式子中，第一部分rT，T表示叶子节点的数量，r是超参，也就是说r越大，那么我们的叶子节点数量就会越小。第二部分是L2正则项，通过对叶子结点的权重进行惩罚，使得不会存在权重过大的叶子结点防止过拟合。w就表示叶子结点的权重。  </p>
<p>但是对于上面这么一个目标函数，我们是很难进行优化的，于是我们将它变换一下，我们通过每一步增加一个基分类器ft，贪婪地去优化这个目标函数，使得每次增加，都使得loss变小。如此一来，我们就得到了一个可以用于评价当前分类器ft性能的一个评价函数：  </p>
<h1 id="-5"><a href="#-5" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-1.jpg" alt=""></h1><p>这个算法又可以称为前向分步优化。为了更快地去优化这个函数，我们可以在ft=0处二阶泰勒展开：  </p>
<h1 id="-6"><a href="#-6" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-2.jpg" alt=""></h1><p>由于我们的目标函数L只受函数f的影响，上一次的loss对本次迭代并没有影响，于是我们可以删除掉常数项。  </p>
<p>误差函数的二阶泰勒展开：  </p>
<h1 id="-7"><a href="#-7" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%EF%BC%881%EF%BC%89.png" alt=""></h1><h1 id="-8"><a href="#-8" class="headerlink" title=""></a><img src="浅析Xgboost原理/（2）.png" alt=""></h1><p>Xgboost的打分函数：  </p>
<h1 id="-9"><a href="#-9" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%EF%BC%882%EF%BC%89.png" alt=""></h1><p>树节点分裂算法–精确算法，遍历所有特征的所有可能的分割点，计算gain值，选取值最大的（feature，value）去分割</p>
<h1 id="-10"><a href="#-10" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E7%B2%BE%E7%A1%AE%E7%AE%97%E6%B3%95.png" alt=""></h1><p>树节点分裂方法–近似算法，对于每个特征，只考察分位点，减少计算复杂度</p>
<h1 id="-11"><a href="#-11" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E8%BF%91%E4%BC%BC%E7%AE%97%E6%B3%95.png" alt=""></h1><p>树节点分裂方法–近似算法举例：三分位数  </p>
<p>实际上Xgboost不是简单地按照样本个数进行分为，而是以二阶导数值作为权重。</p>
<h3 id="Xgboost的其他特性"><a href="#Xgboost的其他特性" class="headerlink" title="Xgboost的其他特性"></a>Xgboost的其他特性</h3><ol>
<li>行抽样（row sample）</li>
<li>列抽样（column sample），借鉴了随机森林</li>
<li>Shrikage（缩减），即学习速率。将学习速率调小，迭代次数增多，有正则化作用</li>
<li>支持自定义损失函数（需二阶可导）</li>
</ol>
<p>Column Block 和 Cache Aware Access：  </p>
<h1 id="-12"><a href="#-12" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1.png" alt=""></h1><h3 id="LightGBM的改进"><a href="#LightGBM的改进" class="headerlink" title="LightGBM的改进"></a>LightGBM的改进</h3><ol>
<li><p>直方图算法：<br>把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。  </p>
<p> 这样减少了内存占用；也减少了split finding时计算增益的计算量。</p>
</li>
<li><p>直方图差加速<br> 一个叶子的直方图可以由它的父亲节点的直方图与它兄弟节点的 直方图做差得到，提升一倍速度。</p>
</li>
<li><p>建树过程的两种方法<br> Level-wise tree growth：同一层所有节点都做分裂，最后剪枝。代表有Xgboost<br> Leaf-wise tree growth：选取具有最大增益节点分裂，容易过拟合，可以通过max_depth限制。代表有LightGBM</p>
</li>
<li><p>并行优化<br> LightGBM的特征并行：<br> 每个worker保存所有数据集<br>（1）每个worker在其特征子集上寻找最佳切分点<br>（2）worker之间互相通信，找到全局最佳切分点<br>（3）每个worker根据全局最佳切分点进行节点分裂  </p>
<p> 优点：避免广播instance indices，减小网络通信量<br> 缺点：split finding计算复杂度没有减小；且当数据量比较大时，单个worker存储所有数据代价高  </p>
</li>
</ol>
<p>传统的数据并行<br>（1）水平切分数据，每个worker只有部分数据<br>（2）每个worker根据本地数据统计局部直方图<br>（3）合并所有局部直方图得到全局直方图<br>（4）根据全局直方图进行节点分裂  </p>
<p>缺点：网络通信代价巨大  </p>
<p>LightGBM的数据并行<br>（1）不同的worker合并不同特征的局部直方图<br>（2）采用直方图做差算法，只需要通信一个节点的直方图</p>
<p>LightGBM其他地方的改进：  </p>
<h1 id="-13"><a href="#-13" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E6%94%B9%E8%BF%9B.png" alt=""></h1><h3 id="Xgboost和GBDT的区别"><a href="#Xgboost和GBDT的区别" class="headerlink" title="Xgboost和GBDT的区别"></a>Xgboost和GBDT的区别</h3><ol>
<li><p>传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。</p>
</li>
<li><p>传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。</p>
</li>
<li><p>xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。</p>
</li>
<li><p>Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）</p>
</li>
<li><p>列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。</p>
</li>
<li><p>对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。</p>
</li>
<li><p>xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</p>
</li>
<li><p>可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。</p>
</li>
</ol>
<h3 id="Xgboost使用经验总结"><a href="#Xgboost使用经验总结" class="headerlink" title="Xgboost使用经验总结"></a>Xgboost使用经验总结</h3><ol>
<li><p>多类别分类时，类别需要从0开始编码</p>
</li>
<li><p>Watchlist不会影响模型训练。</p>
</li>
<li><p>类别特征必须编码，因为xgboost把特征默认都当成数值型的</p>
</li>
<li><p>训练的时候，为了结果可复现，记得设置随机数种子。</p>
</li>
<li><p>XGBoost的特征重要性是如何得到的？某个特征的重要性（feature score），等于它被选中为树节点分裂特征的次数的和，比如特征A在第一次迭代中（即第一棵树）被选中了1次去分裂树节点，在第二次迭代被选中2次…..那么最终特征A的feature score就是 1+2+….</p>
</li>
</ol>
<p>参考链接：<br><a href="http://wepon.me/2016/05/07/XGBoost%E6%B5%85%E5%85%A5%E6%B5%85%E5%87%BA/" target="_blank" rel="noopener">http://wepon.me/2016/05/07/XGBoost%E6%B5%85%E5%85%A5%E6%B5%85%E5%87%BA/</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/GBDT-Gradient-Boosting-Decision-Tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Witton Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WittonZhou">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/GBDT-Gradient-Boosting-Decision-Tree/" itemprop="url">GBDT(Gradient Boosting Decision Tree)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-15T18:16:02+08:00">
                2018-11-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h2><p>优点：效果好；既可以用于分类也可以用于回归；可以筛选特征；可以灵活处理各种类型的数据，包括连续值和离散值。<br>缺点：由于弱学习器之间存在依赖关系，难以并行训练数据。</p>
<p>GBDT又叫MART（Multiple Additive Regression Tree），是一种迭代的决策树算法。GBDT几乎可用于所有回归问题（线性/非线性），相对logistic regression仅能用于线性回归，GBDT的适用面非常广。亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。GBDT无论用于分类还是回归一直都是使用的CART回归树。<br>在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是ft−1(x), 损失函数是L(y,ft−1(x)), 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器ht(x)，让本轮的损失函数L(y,ft(x)=L(y,ft−1(x)+ht(x))最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。  </p>
<h3 id="1-Regression-Decision-Tree：回归树"><a href="#1-Regression-Decision-Tree：回归树" class="headerlink" title="1.Regression Decision Tree：回归树"></a>1.Regression Decision Tree：回归树</h3><p>回归树总体流程类似于分类树，区别在于，回归树的每一个节点都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化平方误差（最小化均方差）。也就是被预测出错的人数越多，错的越离谱，平方误差就越大，通过最小化平方误差能够找到最可靠的分枝依据。分枝直到每个叶子节点上人的年龄都唯一或者达到预设的终止条件(如叶子个数上限)，若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。  </p>
<p>回归树算法如下图（截图来自《统计学习方法》5.5.1 CART生成）： </p>
<h1 id=""><a href="#" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E5%9B%9E%E5%BD%92%E6%A0%91.png" alt=""></h1><h3 id="2-Boosting-Decision-Tree：提升树算法"><a href="#2-Boosting-Decision-Tree：提升树算法" class="headerlink" title="2.Boosting Decision Tree：提升树算法"></a>2.Boosting Decision Tree：提升树算法</h3><p>提升树是迭代多棵回归树来共同决策。当采用平方误差损失函数时，每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树，残差的意义如公式：残差 = 真实值 - 预测值 。提升树即是整个迭代过程生成的回归树的累加。  </p>
<p>示例如下：</p>
<h1 id="-1"><a href="#-1" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95.png" alt=""></h1><p>提升树算法：  </p>
<h1 id="-2"><a href="#-2" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E6%8F%90%E5%8D%87%E6%A0%91.png" alt=""></h1><h3 id="3-GBDT：梯度提升决策树"><a href="#3-GBDT：梯度提升决策树" class="headerlink" title="3.GBDT：梯度提升决策树"></a>3.GBDT：梯度提升决策树</h3><p>利用最速下降的近似方法，即利用损失函数的负梯度在当前模型的值，作为回归问题中提升树算法的残差的近似值，拟合一个回归树。<br>算法如下：  </p>
<h1 id="-3"><a href="#-3" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-GBDT.png" alt=""></h1><p>I(x∈Rjm)其实就是一个很简单的函数，如果x∈Rtj，那么I(x∈Rjm)=1，否则等于0。</p>
<p>算法步骤解释：  </p>
<ol>
<li>初始化弱学习器f0，估计使损失函数极小化的常数值，它是只有一个根节点的树，即ganma是一个常数值。  </li>
<li>对迭代轮数m=1~M有<br>（a）计算损失函数的负梯度在当前模型的值，将它作为残差的估计<br>（b）估计回归树叶节点区域，以拟合残差的近似值<br>（c）利用线性搜索估计叶节点区域的值，使损失函数极小化<br>（d）更新回归树（更新强学习器）  </li>
<li>得到输出的最终模型 f(x)（得到强学习器f（x）的表达式）  </li>
</ol>
<p>注：ganma是GBDT中每颗决策树拟合叶子节点最好的输出值，在回归GBDT初始化的时候，ganma一般取值为所有y的平均值。  </p>
<p>当我们迭代到第t个弱分类器的时候，我们把损失函数进行一阶泰勒展开，出现的f_t-1（x）+ △x 的形式，这个△x 就是我们这一轮弱分类器想要输出的值，一阶泰勒展开后面有一项是导数和△x的乘积。我们当然想让这一轮的损失函数小于上一轮的，那我们就让△x等于负导数 ，这样一个值的平方加上负号，一定是一个负值，这样就能保证这一轮的损失函数小于上一轮的。</p>
<p>GBDT回归算法：</p>
<h1 id="-4"><a href="#-4" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-15%20%E4%B8%8B%E5%8D%889.19.34.png" alt=""></h1><p>GBDT二分类算法：</p>
<h1 id="-5"><a href="#-5" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-15%20%E4%B8%8B%E5%8D%889.20.31.png" alt=""></h1><h3 id="4-重要参数的意义和设置"><a href="#4-重要参数的意义和设置" class="headerlink" title="4.重要参数的意义和设置"></a>4.重要参数的意义和设置</h3><p>推荐GBDT树的深度：6（横向比较：DecisionTree/RandomForest需要把树的深度调到15或更高）<br>以下摘自知乎上的一个问答（详见参考文献8），问题和回复都很好的阐述了这个参数设置的数学原理。<br>【问】xgboost/gbdt在调参时为什么树的深度很少就能达到很高的精度？  </p>
<p>用xgboost/gbdt在在调参的时候把树的最大深度调成6就有很高的精度了。但是用DecisionTree/RandomForest的时候需要把树的深度调到15或更高。用RandomForest所需要的树的深度和DecisionTree一样我能理解，因为它是用bagging的方法把DecisionTree组合在一起，相当于做了多次DecisionTree一样。但是xgboost/gbdt仅仅用梯度上升法就能用6个节点的深度达到很高的预测精度，使我惊讶到怀疑它是黑科技了。请问下xgboost/gbdt是怎么做到的？它的节点和一般的DecisionTree不同吗？  </p>
<p>【答】<br>  这是一个非常好的问题，题主对各算法的学习非常细致透彻，问的问题也关系到这两个算法的本质。这个问题其实并不是一个很简单的问题，我尝试用我浅薄的机器学习知识对这个问题进行回答。  </p>
<p>一句话的解释，来自周志华老师的机器学习教科书（机器学习-周志华）：Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成；Bagging主要关注降低方差，因此它在不剪枝的决策树、神经网络等学习器上效用更为明显。</p>
<p>随机森林(random forest)和GBDT都是属于集成学习（ensemble learning)的范畴。集成学习下有两个重要的策略Bagging和Boosting。  </p>
<p>Bagging算法是这样做的：每个分类器都随机从原样本中做有放回的采样，然后分别在这些采样后的样本上训练分类器，然后再把这些分类器组合起来。简单的多数投票一般就可以。其代表算法是随机森林。Boosting的意思是这样，他通过迭代地训练一系列的分类器，每个分类器采用的样本分布都和上一轮的学习结果有关。其代表算法是AdaBoost, GBDT。  </p>
<p>其实就机器学习算法来说，其泛化误差可以分解为两部分，偏差（bias)和方差(variance)。这个可由下图的式子导出（这里用到了概率论公式D(X)=E(X2)-[E(X)]2）。偏差指的是算法的期望预测与真实预测之间的偏差程度，反应了模型本身的拟合能力；方差度量了同等大小的训练集的变动导致学习性能的变化，刻画了数据扰动所导致的影响。这个有点儿绕，不过你一定知道过拟合。  </p>
<p>如下图所示，当模型越复杂时，拟合的程度就越高，模型的训练偏差就越小。但此时如果换一组数据可能模型的变化就会很大，即模型的方差很大。所以模型过于复杂的时候会导致过拟合。  </p>
<p>当模型越简单时，即使我们再换一组数据，最后得出的学习器和之前的学习器的差别就不那么大，模型的方差很小。还是因为模型简单，所以偏差会很大。  </p>
<p>模型复杂度与偏差方差的关系图：  </p>
<h1 id="-6"><a href="#-6" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-6D27F5B9-8202-473B-A692-C0D6AEB309D8.png" alt=""></h1><p>也就是说，当我们训练一个模型时，偏差和方差都得照顾到，漏掉一个都不行。  </p>
<p>对于Bagging算法来说，由于我们会并行地训练很多不同的分类器的目的就是降低这个方差(variance) ,因为采用了相互独立的基分类器多了以后，h的值自然就会靠近.所以对于每个基分类器来说，目标就是如何降低这个偏差（bias),所以我们会采用深度很深甚至不剪枝的决策树。  </p>
<p>对于Boosting来说，每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差（bias）,所以对于每个基分类器来说，问题就在于如何选择variance更小的分类器，即更简单的分类器，所以我们选择了深度很浅的决策树。</p>
<h3 id="5-Shrinkage"><a href="#5-Shrinkage" class="headerlink" title="5.Shrinkage"></a>5.Shrinkage</h3><p>Shrinkage（缩减）的思想认为，每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它不完全信任每一个棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。  </p>
<p>没用Shrinkage时：（yi表示第i棵树上y的预测值， y(1~i)表示前i棵树y的综合预测值）<br>y(i+1) = 残差(y1~yi)， 其中： 残差(y1~yi) =  y真实值 - y(1 ~ i)<br>y(1 ~ i) = SUM(y1, …, yi)  </p>
<p>Shrinkage不改变第一个方程，只把第二个方程改为：<br>y(1 ~ i) = y(1 ~ i-1) + step * yi</p>
<p>即Shrinkage仍然以残差作为学习目标，但对于残差学习出来的结果，只累加一小部分（step*残差）逐步逼近目标，step一般都比较小，如0.01~0.001（注意该step非gradient的step），导致各个树的残差是渐变的而不是陡变的。直觉上这也很好理解，不像直接用残差一步修复误差，而是只修复一点点，其实就是把大步切成了很多小步。本质上，Shrinkage为每棵树设置了一个weight，累加时要乘以这个weight，但和Gradient并没有关系。</p>
<h3 id="6-总结"><a href="#6-总结" class="headerlink" title="6.总结"></a>6.总结</h3><p>GBDT主要是通过损失函数的负梯度来拟合本轮损失的近似值（也就是本轮残差的近似值），进而拟合一个CART回归树。根据第t棵回归树，得到对应的叶节点区域。然后根据叶子区域，计算得到最佳拟合值，然后根据更新公式来更新学习器。<br>此外，可以看出Boosting算法是一种加法模型。</p>
<p>参考链接：<br><a href="https://www.jianshu.com/p/005a4e6ac775" target="_blank" rel="noopener">https://www.jianshu.com/p/005a4e6ac775</a><br><a href="https://www.cnblogs.com/pinard/p/6140514.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6140514.html</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/读《解析卷积神经网络》/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Witton Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WittonZhou">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/读《解析卷积神经网络》/" itemprop="url">读《解析卷积神经网络》</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-13T15:33:01+08:00">
                2018-11-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="《解析卷积神经网络–深度学习实践手册》-作者魏秀参"><a href="#《解析卷积神经网络–深度学习实践手册》-作者魏秀参" class="headerlink" title="《解析卷积神经网络–深度学习实践手册》 作者魏秀参"></a>《解析卷积神经网络–深度学习实践手册》 作者魏秀参</h2><hr>
<h3 id="1-CNN基本部件"><a href="#1-CNN基本部件" class="headerlink" title="1.CNN基本部件"></a>1.CNN基本部件</h3><p>1.1 深度学习的“端到端”学习方式  </p>
<p>深度学习的一个重要思想“端到端”的学习方式，属于表示学习的一种。其他机器学习算法，如特征选择算法、分类器算法、集成学习算法等，均假设样本特征表示是给定的，并在此基础上设计具体的机器学习算法。在深度学习时代之前，样本表示基本都使用人工特征，但是人工特征的优劣往往很大程度决定了最终的任务精度。而在深度学习普及之后，人工特征已逐渐被表示学习根据任务自动需求“学到”的特征表示所取代。  </p>
<p>过去解决一个人工智能问题（以图像识别为例）往往通过分治法将其分解为预处理、特征提取与选择、分类器设计等若干步骤。子问题上的最优解并不意味着就能得到全局问题的最后解。深度学习为我们提供了另一种学习方式，即“端到端”学习方式，整个学习流程并不进行人为的子问题划分，而是完全交给深度学习模型直接学习从原始输入到期望输出的映射。  </p>
<p>1.2 卷积层<br>卷积层是卷积神经网络中的基础操作，甚至在网络最后起分类作用的全连接层在工程实现时也是由卷积操作替代的。 卷积层有一个独特的特征：“权值共享”特征。<br>卷积操作的两个重要的超参数：卷积核大小（filter size）和卷积步长（stride）。  </p>
<p>1.3 池化（pooling）层<br>文中翻译为汇合层。<br>通常使用的汇合操作为平均值汇合和最大值汇合，还有随机汇合介于二者之间。<br>汇合操作实际上就是一种“降采样”操作，另一方面，汇合也看成是一个用p-范数作为非线性映射的“卷积”操作，当p趋近正无穷时就是最常见的最大值汇合。<br>汇合层的引入是仿照人的视觉系统对视觉输入对象进行降维（降采样）和抽象。汇合层有以下三种功效：<br>1.特征不变性。汇合操作时模型更关注是否存在某些特征而不是特征具体的位置，使特征学习包含某种程度自由度，能容忍一些特征微小的位移。<br>2.特征降维。汇合相当于空间范围内做了维度约减，从而使模型可以抽取更广范围的特征。同时减小了下一层输入大小，进而减少计算量和参数个数。<br>3.在一定程度上防止过拟合，更方便优化。  </p>
<p>汇合操作并不是CNN必须的元件或操作。  </p>
<p>1.4 激活函数<br>激活函数的引入为的是增加整个网络的表达能力（即非线性）。<br>Sigmoid型函数将输出响应的值域被压缩到[0,1]之间，0对应了生物神经元的“抑制状态”，1则对应了“兴奋状态”。Sigmoid有一个严重的问题，即梯度的“饱和效应”。在输出值接近1时，梯度接近0，这会导致在误差反向传播过程中导数处于该区域的误差将很难甚至根本无法传递到前层，进而导致整个网络无法训练。<br>ReLU函数是目前深度卷积神经网络中最为常用的激活函数之一。ReLU函数的梯度在x&gt;=0时为1，反之为0。对x&gt;=0部分完全消除了Sigmoid型函数的梯度饱和效应。ReLU函数还有助于随机梯度下降方法收敛，收敛速度约快6倍左右。  </p>
<p>1.5 全连接层<br>全连接层起到将学到的特征表示映射到样本的标记空间的作用。  </p>
<p>1.6 目标函数<br>目标函数的作用是用来衡量该预测值与真实样本标记之间的误差。  </p>
<h1 id=""><a href="#" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B%E5%9B%BE.png" alt=""></h1><h3 id="2-CNN经典结构"><a href="#2-CNN经典结构" class="headerlink" title="2.CNN经典结构"></a>2.CNN经典结构</h3><ol>
<li><p>感受野<br>随着网络深度的加深，后层神经元在第一层输入层的感受野会随之增大。也就是说，小卷积核通过多层叠加可取得与大卷积核同等规模的感受野。<br>小卷积核的优势：<br>（1）由于小卷积核需要多层叠加，加深了网络深度进而增强了网络容量<br>（2）增强网络容量的同时减少了参数个数  </p>
</li>
<li><p>残差网络模型<br>神经网络的深度和宽度是表征网络复杂度的两个核心因素，不过深度相比宽度在增加网络的复杂度方面更有效。然而，随着深度的增加，训练会变得愈加困难。这主要因为在基于随机梯度下降的网络训练过程中，误差信号的多层反向传播非常容易引发梯度“弥散”或者“爆炸”。一些特殊的权重初始化策略和批规范化（batch normalization）可以使这个问题得到极大的改善，但是效果有限。<br>当深度网络收敛时，另外的问题又随之而来：随着继续增加网络的深度，训练数据的训练误差没有降低反而升高。残差网络很好的解决了这个问题。  </p>
</li>
</ol>
<h3 id="3-卷积神经网络的压缩"><a href="#3-卷积神经网络的压缩" class="headerlink" title="3.卷积神经网络的压缩"></a>3.卷积神经网络的压缩</h3><p>按照压缩过程对网络结构的破坏程度，我们将模型压缩技术分为“前端压缩”与“后端压缩”两部分。所谓“前端压缩”，是指不改变原网络结构的压缩技术，主要包括知识蒸馏、紧凑的模型结构设计以及滤波器层面的剪枝等；而“后端压缩”则包括低秩近似、未加限制的剪枝、参数量化以及二值网络等，其目标在于尽可能地减少模型大小，因而会对原始网络结构造成极大程度的改造。  </p>
<p>低秩近似、剪枝与参数量化作为常用的三种压缩技术，已经具备了较为明朗的应用前景；其他压缩技术，如二值网络、知识蒸馏等尚处于发展阶段。  </p>
<ol>
<li><p>低秩近似<br> 使用结构化矩阵来进行低秩分解的算法；也可以直接使用矩阵分解来降低权重矩阵的参数。低秩近似算法在中小型网络模型上取得了很不错的效果，但其超参数量与网络层数呈线性变化趋势，随着网络层数的增加与模型复杂度的提升，其搜索空间会急剧增大。  </p>
</li>
<li><p>剪枝与稀疏约束<br> 给定一个预训练好的网络模型，常用的剪枝算法一般都遵从如下的操作流程：<br> （1）衡量神经元的重要程度。衡量其重要程度的方法也是多种多样，从一些基本的启发式算法，到基于梯度的方案，其计算复杂度与最终的效果也是各有千秋。<br> （2）移除掉一部分不重要的神经元。这里可以根据某个阈值来判断神经元是否可以被剪除，也可以按重要程度排序，剪除掉一定比例的神经元。一般而言，后者比前者更加简便，灵活性也更高。<br> （3）对网络进行微调。由于剪枝操作会不可避免地影响网络的精度，为防止对分类性能造成过大的破坏，需要对剪枝后的模型进行微调。<br> （4）返回第一步，进行下一轮剪枝  </p>
</li>
<li><p>参数量化<br> 最简单也是最基本的一种量化算法便是标量量化。该算法的基本思路是，对于每一个权重矩阵，首先将其转化为向量形式。之后对该权重向量的元素进行 k 个簇的聚类，这可借助于经典的k均值聚类算法快速完成。如此一来，只需将 k 个聚类中心（标量）存储在码本之中便可，而原权重矩阵则只负责记录各自聚类中心在码本中的索引。  </p>
</li>
<li><p>二值网络<br> 二值网络可以被视为量化方法的一种极端情况：所有参数的取值只能是±1。正是这种极端的设定，使得二值网络能够获得极大的压缩效益。  </p>
<p> 首先，在普通的神经网络中，一个参数是由单精度浮点数来表示的，参数的二值化能将存储开销降低为原来的1/32。其次，如果中间结果也能二值化的话，那么所有的运算仅靠位操作便可完成。借助于同或门等逻辑门元件便能快速完成所有的计算。而这一优点是其余压缩方法所不能比拟的。深度神经网络的一大诟病就在于其巨大的计算代价，如果能够获得高准确度的二值网络，那么便可摆脱对GPU等高性能计算设备的依赖。  </p>
</li>
<li><p>知识蒸馏<br> 在不改变模型复杂度的情况下，通过增加监督信息的丰富程度，带来性能上的提升。  </p>
</li>
</ol>
<ol start="6">
<li>紧凑的网络结构<br> 直接训练小模型。设计上不容易，依赖经验和技巧。随着参数数量的降低，网络的泛化性不一定能得到保障。 </li>
</ol>
<h3 id="4-数据扩充"><a href="#4-数据扩充" class="headerlink" title="4.数据扩充"></a>4.数据扩充</h3><p>数据扩充是深度模型训练前的必须一步，此操作可扩充训练数据集，增强数据多样性，防止模型过拟合。<br>简单的数据扩充方式有：水平翻转，随机扣取，尺度变换，旋转等; 对原图或已变换的图像进行色彩抖动。<br>实践中，往往会将上述几种方式叠加使用，如此可将图像数据扩充至原有数据的数倍甚至数十倍。  </p>
<p>特殊的数据扩充方式：  </p>
<ol>
<li>Fancy PCA  </li>
<li>监督式数据扩充  </li>
</ol>
<h3 id="5-数据预处理"><a href="#5-数据预处理" class="headerlink" title="5.数据预处理"></a>5.数据预处理</h3><p>机器学习中， 对于输入特征做归一化预处理操作是常见的步骤。类似的，在图像处理中，图像的每个像素信息同样可以看做一种特征。在实践中，对每个特征减去平均值来中心化数据是非常重要的，这种归一化处理方式被称作“中心式归一化”（mean normalization）。<br>减均值操作的原理是，我们默认自然图像是一类平稳的数据分布（即数据每一个维度的统计都服从相同分布），此时，在每个样本上减去数据的统计平均值（逐样本计算）可以移除共同部分，凸显个体差异。<br>需要注意的是，在实际操作中应首先划分好训练集、验证集和测试集，而该均值仅针对划分后的训练集计算。不可直接在未划分的所有图像上计算均值，如此会违背机器学习基本原理，即“模型训练过程中能且仅能从训练数据中获取信息”。  </p>
<h3 id="6-网络参数初始化"><a href="#6-网络参数初始化" class="headerlink" title="6.网络参数初始化"></a>6.网络参数初始化</h3><p>理想的网络参数初始化使模型训练事半功倍，相反，糟糕的初始化方案不仅会影响网络收敛甚至会导致梯度弥散或爆炸致使训练失败。介绍几种目前实践中常用的几种网络参数初始化方式。  </p>
<ol>
<li><p>全零初始化<br> 当网络收敛到稳定状态时，参数（权值）在理想情况下应基本保持正负各半的状态（此时期望为 0）。因此，一种简单且听起来合理的参数初始化做法是，干脆将所有参数都初始化为0，因为这样可使得初始化全零时参数的期望与网络稳定时参数的期望一致为零。  </p>
<p> 不过，细细想来则会发现参数全为0时网络不同神经元的输出必然相同，相同输出则导致梯度更新完全一样，这样便会令更新后的参数仍然保持一样的状态。换句话说，如若参数进行了全零初始化，那么网络神经元将毫无能力对此做出改变，从而无法进行模型训练。</p>
</li>
<li><p>随机初始化  </p>
<p> 将参数随机化自然是打破上述“僵局”的一个有效手段，不过我们仍然希望所有参数期望依旧接近0。遵循这一原则，我们可将参数值随机设定为接近0的一个很小的随机数（有正有负）。在实际应用中，随机参数服从高斯分布或均匀分布都是较有效的初始化方式。还有一个问题，网络输出数据分布的方差会随着输入神经元个数改变。所以会在初始化的同时加上方差大小的规范化。  </p>
</li>
<li><p>其他初始化方法  </p>
<p> 利用预训练模型—-将预训练模型的参数作为新任务上模型的参数初始化。</p>
</li>
</ol>
<h3 id="7-激活函数"><a href="#7-激活函数" class="headerlink" title="7.激活函数"></a>7.激活函数</h3><p>激活函数又称为“非线性映射函数”。以下将介绍七种当下深度卷积神经网络中常用的激活函数：<br>1.Sigmoid型函数  </p>
<h1 id="-1"><a href="#-1" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-sigmoid.png" alt=""></h1><p>Sigmoid型函数值域的均值并非为0而是全为正，这样的结果实际上并不符合我们队神经网络内数值的期望（均值）应为0的设想。</p>
<p>2.tanh(x)型函数  </p>
<h1 id="-2"><a href="#-2" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-tanh-x-.png" alt=""></h1><p>tanh(x)型函数是在Sigmoid型函数基础上为解决均值问题提出的激活函数。tanh(x)型函数又称作双曲正切函数，其函数范围是(-1,+1)，输出响应的均值为0。但是tanh(x)型函数仍基于Sigmoid型函数，依然会发生“梯度饱和”现象。  </p>
<p>3.修正线性单元(ReLU)  </p>
<h1 id="-3"><a href="#-3" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-ReLU.png" alt=""></h1><p>ReLU函数是目前深度卷积神经网络中最为常用的激活函数之一。<br>对 x ≥ 0 部分完全消除了Sigmoid型函数的梯度饱和效应。但是也存在缺陷，即在 x &lt; 0 时，梯度便为0。对于小于0的这部分卷积结果响应，它们一旦变为负值将再无法影响网络训练——这种现象被称作“死区”。  </p>
<p>4.Leaky ReLU<br>为了缓解“死区”现象，研究者将ReLU函数中 x &lt; 0 的部分调整为 f(x) = α·x， 其中 α 为 0.01或0.001数量级的较小正数。  </p>
<h1 id="-4"><a href="#-4" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-Leaky%20ReLU.png" alt=""></h1><p>原始的ReLU函数实际上是Leaky ReLU函数的一个特例。不过由于Leaky ReLU中 α 为超参数，合适的值较难设定且较为敏感，因此Leaky ReLU函数在实际使用中的性能并不十分稳定。  </p>
<p>5.参数化ReLU<br>参数化ReLU的提出很好的解决了Leaky ReLU中超参数 α 不易设定的问题：参数化ReLU直接将 α 也作为一个网络中可学习的变量融入模型的整体训练过程。<br>参数化ReLU在带来更大自由度的同时，也增加了网络模型过拟合的风险，在实际使用中需格外注意。  </p>
<p>6.随机化ReLU<br>另一种解决 α 超参设定的方式是将其随机化，这便是随机化ReLU。  </p>
<p>7.指数化线性单元(ELU)  </p>
<h1 id="-5"><a href="#-5" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-ELU.png" alt=""></h1><p>ELU具备ReLU函数的优点， 同时ELU也解决了ReLU函数自身的“死区”问题。不过，ELU函数中的指数操作稍稍增大了计算量。实际使用中，ELU中的超参数 λ 一般设置为1。  </p>
<h3 id="8-目标函数"><a href="#8-目标函数" class="headerlink" title="8.目标函数"></a>8.目标函数</h3><ol>
<li><p>交叉熵损失函数<br> 又称为Softmax损失函数，是目前卷积神经网络中最常用的分类目标函数，通过指数化变换使网络输出h转换为概率形式。<br> <img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png" alt=""></p>
</li>
<li><p>合页损失函数<br> 在SVM中被广泛使用。<br> <img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E5%90%88%E9%A1%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png" alt=""><br> 一般情况的分类任务中，交叉熵损失函数的分类效果略优于合页损失函数的分类效果。  </p>
</li>
<li><p>坡道损失函数  </p>
</li>
<li>大间隔交叉熵损失函数</li>
<li>中心损失函数  </li>
</ol>
<p>总结：<br>交叉熵损失函数是最为常用的分类目标函数，且效果一般优于合页损失函数；<br>大间隔损失函数和中心损失函数从增大类间距离、减小类内距离的角度不仅要求分类准确，而且还有助提高特征的分辨能力；<br>坡道损失函数是分类问题目标函数中的一类非凸损失函数，由于其良好的抗噪特性，推荐将其用于样本噪声或离群点较多的分类任务。  </p>
<h3 id="9-网络正则化"><a href="#9-网络正则化" class="headerlink" title="9.网络正则化"></a>9.网络正则化</h3><p>机器学习的一个核心问题是如何使学习算法不仅在训练样本上表现良好，并且在新数据或测试集上同样奏效，学习算法在新数据上的这样一种表现我们称之为模型的“泛化能力”。若某学习算法在训练集表现优异，同时在测试集依然表现良好，可以说该学习算法有较强泛化能力。  </p>
<p>1.L2正则化<br>L2正则化方式在深度学习中有个常用的叫法是“权重衰减”， 另外L2正则化在机器学习中还被称作“岭回归”或Tikhonov正则化。  </p>
<p>2.L1正则化<br>需注意，L1正则化除了同L2正则化一样能约束参数量级外，L1正则化还能起到使参数更稀疏的作用。稀疏化的结果使优化后的参数一部分为 ，另一部分为非零实值。非零实值的那部分参数可起到选择重要参数或特征维度的作用， 同时可起到去除噪声的效果。  </p>
<p>3.最大范数约束<br>最大范数约束是通过向参数量级的范数设置上限对网络进行正则化的手段。  </p>
<p>4.dropout(随机失活)<br>随机失活的提出正是一定程度上缓解了神经元之间复杂的协同适应，降低了神经元间依赖，避免了网络过拟合的发生。其原理非常简单：对于某层的每个神经元，在训练阶段均以概率 p 随机将该神经元权重置0（故被称作“随机失活”），测试阶段所有神经元均呈激活态，但其权重需乘 (1 − p) 以保证训练和测试阶段各自权重拥有相同的期望。  </p>
<p>由于失活的神经元无法参与到网络训练，因此每次训练（前向操作和反向操作）时相当于面对一个全新网络。以含两层网络、各层有三个神经元的简单神经网络为例，若每层随机失活一个神经元，该网络共可产生9种子网络。根据上述随机失活原理，训练阶段相当于共训练了9个子网络，测试阶段则相当于9个子网络的平均集成。  </p>
<h1 id="-6"><a href="#-6" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-11-14%20%E4%B8%8B%E5%8D%887.57.09.png" alt=""></h1><p>5.验证集的使用<br>借助验证集对网络训练“早停”是一种有效的防止网络过拟合方法。</p>
<h3 id="10-超参数设定和网络训练"><a href="#10-超参数设定和网络训练" class="headerlink" title="10.超参数设定和网络训练"></a>10.超参数设定和网络训练</h3><ol>
<li><p>输入数据像素大小<br> 统一将图像压缩到2^n大小，便于GPU设备并行。  </p>
</li>
<li><p>卷积层参数的设定<br> 包括卷积核大小、卷积操作的步长和卷积核个数。<br> 实践中推荐使用3 <em> 3以及 5 </em> 5，其对应卷积操作步长为1。<br> 为了硬件字节级存储管理的方便，卷积核个数通常设置为2 的次幂，如64，128，512等。  </p>
</li>
<li>汇合层参数的设定<br> 常用的参数设定为2 <em> 2、汇合步长为2，起到“降采样”的作用。为了不丢弃过多输入相应而损失网络性能，汇合操作极少使用超过3 </em> 3大小的汇合操作。  </li>
</ol>
<hr>
<p>训练技巧  </p>
<ol>
<li>训练数据随机打乱<br> 在训练卷积神经网络时，尽管训练数据固定，但由于采用了随机批处理（mini-batch）的训练机制，因此我们可在模型每轮（epoch）训练进行前将训练数据集随机打乱（shuffle），确保模型不同轮数相同批次“看到”的数据是不同的。  </li>
<li>学习率的设定<br> （1）模型训练开始时的初始学习率不宜过大，以0.01和0.001为宜<br> （2）模型训练过程中，学习率应随轮数增加而减缓。也可以分为，轮数减缓、指数减缓、分数减缓。  </li>
<li><p>批规范操作（BN）<br> 在模型每次随机梯度下降训练时，通过mini-batch来对相应的网络响应做规范化操作，使得结果（输出信号各个维度）的均值为0，方差为1。算法如下：<br>  <img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-BN.png" alt=""></p>
<p> BN奏效的原因：在统计机器学习中的一个经典假设是“源空间和目标空间的数据分布是一致的”。如果不一致，那么就出现了新的机器学习问题，如迁移学习等。通过BN来规范化某些层或所有层的输入，从而可以固定每层输入信号的均值与方差。这样一来，即使网络模型较深层的响应或梯度很小，也可通过BM的规范化作用将其的尺度变大，以此便可解决深层网络训练很可能带来的“梯度弥散” 问题。  </p>
</li>
<li>网络模型优化算法选择<br> （1）随机梯度下降法。经典的随机梯度下降是最常见的神经网络优化方法，收敛效果较稳定，不过收敛速度过慢。<br> （2）基于动量的随机梯度下降法。<br> （3）Nesterov型动量随机下降法。<br> （4）Adagrad法。<br> （5）Adadelta法。<br> （6）RMSProp法。<br> （7）Adam法。  </li>
<li>微调神经网络<br> 微调预训练模型简单来说，就是用目标任务数据在原先预训练模型上继续进行训练过程。微调预训练模型时，需注意学习率调整和原始数据与目标数据的关系。另外，还可使用“多目标学习框架”对预训练模型进行微调。  </li>
</ol>
<h3 id="后面的几章涉及到的都是基础内容，就不做过多介绍。"><a href="#后面的几章涉及到的都是基础内容，就不做过多介绍。" class="headerlink" title="后面的几章涉及到的都是基础内容，就不做过多介绍。"></a>后面的几章涉及到的都是基础内容，就不做过多介绍。</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/23岁，被谈及最多的问题是钱/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Witton Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WittonZhou">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/23岁，被谈及最多的问题是钱/" itemprop="url">23岁，被谈及最多的问题是钱</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-10T10:31:56+08:00">
                2018-11-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/杂谈/" itemprop="url" rel="index">
                    <span itemprop="name">杂谈</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id=""><a href="#" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E4%B8%80%E4%BD%8D%E4%B9%98%E5%AE%A2%E7%AA%81%E7%84%B6%E5%A4%B1%E5%8E%BB%E4%BA%86%E6%A2%A6%E6%83%B3.jpg" alt=""></h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/读论文-OSIBD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Witton Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WittonZhou">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/读论文-OSIBD/" itemprop="url">读论文:OSIBD</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-04T15:21:06+08:00">
                2018-11-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="《A-Novel-Technique-on-Class-Imbalance-Big-Data-using-Analogous-over-Sampling-Approach》-2017-IJCIR"><a href="#《A-Novel-Technique-on-Class-Imbalance-Big-Data-using-Analogous-over-Sampling-Approach》-2017-IJCIR" class="headerlink" title="《A Novel Technique on Class Imbalance Big Data using Analogous over Sampling Approach》,2017,IJCIR"></a>《A Novel Technique on Class Imbalance Big Data using Analogous over Sampling Approach》,2017,IJCIR</h2><p>大数据中的类别不平衡问题降低了现有的分类器的效果，本文提出了一种新奇的算法，叫Over Sampling on Imbalance Big Data(OSIBD)。结果是，OSIBD算法在处理不平衡类别的问题上效果要比C4.5算法好。</p>
<p>类别不平衡问题影响分类器效果的原因是：只有少数的minority 实例是模型构造过程中可以获取的，所以模型是很难预测unseen minority 实例。</p>
<h3 id="处理类别不平衡的方法有"><a href="#处理类别不平衡的方法有" class="headerlink" title="处理类别不平衡的方法有"></a>处理类别不平衡的方法有</h3><p>1.作用于算法的内部方法<br>调整决策threshold，对少数类产生bias，在学习过程中引入costs来补偿少数群体。</p>
<p>2.作用于数据的外部方法<br>对少数类的过采样和对多数类的欠采样。</p>
<p>3.基于boosting的针对训练集不平衡的综合方法</p>
<h3 id="OSIBD算法"><a href="#OSIBD算法" class="headerlink" title="OSIBD算法"></a>OSIBD算法</h3><p>(1)Preparation of the Majority and Minority subsets<br>准备多数类和少数类的子集</p>
<p>(2)Improve with in class imbalances by removing noisy and borderline<br>instances<br>通过去除噪声和边界实例来改善类别不平衡</p>
<p>为了找到弱实例，其中一种方法是找到最有影响属性或特征，然后移除与该特征相关的噪声或弱属性的范围。</p>
<p>(3)Applying oversampling on the minority subset<br>对少数类进行过采样</p>
<p>通过生成合成实例，副本实例和生成具有现有和合成实例的特征的混合实例，以类似方法对预处理的少数子集进行过采样。</p>
<p>(4)Forming the strong dataset<br>形成足够的数据集</p>
<h3 id="个人心得"><a href="#个人心得" class="headerlink" title="个人心得"></a>个人心得</h3><p>文章中提到的过采样算法比较模糊，没有类似SMOTE的具体的过采样算法。<br>OSIBD的实验对比是和C4.5来进行的，而且是单颗树的。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/读论文-FFM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Witton Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WittonZhou">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/读论文-FFM/" itemprop="url">读论文:FFM</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-18T22:18:45+08:00">
                2018-10-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="《Field-aware-Factorization-Machines-for-CTR-Prediction》-2016"><a href="#《Field-aware-Factorization-Machines-for-CTR-Prediction》-2016" class="headerlink" title="《Field-aware Factorization Machines for CTR Prediction》,2016"></a>《Field-aware Factorization Machines for CTR Prediction》,2016</h2><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>在FM模型中，每一个特征会对应一个隐变量，但在FFM模型中，认为应该将特征分为多个field，每个特征对应每个field分别有一个隐变量。  </p>
<p>举个例子，我们的样本有3种类型的字段：publisher, advertiser, gender，分别可以代表媒体，广告主或者是具体的商品，性别。其中publisher有5种数据，advertiser有10种数据，gender有男女2种，经过one-hot编码以后，每个样本有17个特征，其中只有3个特征非空。  </p>
<p>如果使用FM模型，则17个特征，每个特征对应一个隐变量。<br>如果使用FFM模型，则17个特征，每个特征对应3个隐变量，即每个类型对应一个隐变量，具体而言，就是对应publisher, advertiser, gender三个field各有一个隐变量。</p>
<p>本文的成果如下：  </p>
<ul>
<li><p>尽管FFM被证明是有效的，但这项工作可能是唯一一项将FFM应用于CTR预测问题的已发表的研究。 为了进一步证明FFM对CTR预测的有效性，我们提出使用FFM作为我们的主要模型，以赢得由Criteo和Avazu主办的两场全球CTR比赛。  </p>
</li>
<li><p>我们将FFM与两个相关模型Poly2和FM进行比较。我们首先在概念上讨论为什么FFM可能比它们更好，并进行实验以查看准确性和训练时间方面的差异。  </p>
</li>
<li><p>我们提出了训练FFM的技术。它们包括用于FFM的有效并行优化算法以及使用早停以避免过拟合。  </p>
</li>
<li><p>为了使FFM可供公众使用，我们发布了一个开源软件。  </p>
</li>
</ul>
<h3 id="PLOY2和FM"><a href="#PLOY2和FM" class="headerlink" title="PLOY2和FM"></a>PLOY2和FM</h3><p>2次多项式映射通常可以有效地捕获特征交互的信息。此外，他们表明，通过在2维映射的显式形式上应用线性模型，训练和测试时间可以比使用核方法快得多。这种方法称为Poly2，可以学习每个特征对的权重： </p>
<h1 id=""><a href="#" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-12-11%20%E4%B8%8B%E5%8D%883.01.19.png" alt=""></h1><p>其中h（j 1，j 2）是将j 1和j 2编码为自然数的函数。计算的复杂度是O（n_mean^2），其中，n_mean是每个实例的非零元素的平均数。  </p>
<p>之前提出的FM隐含地为每个特征学习隐向量。每个隐在向量包含k个隐因子，其中k是用户指定的参数。然后，特征交互的效果由两个隐向量的内积建模：</p>
<h1 id="-1"><a href="#-1" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-12-11%20%E4%B8%8B%E5%8D%883.04.29.png" alt=""></h1><p>变量的数量是n×k，因此直接计算花费O（n_mean^2 k）时间。化简后为O（n_mean k）。<br>当数据集稀疏时FM可能比Poly2更好。请注意，在Poly2中，实现h（j 1，j 2）的简单方法是将每对特征视为一个新特征。这种方法需要与O（n^2）一样大的模型，由于n非常大，这对于CTR预测通常是不切实际的。<br>在本文中，为了简化公式，我们不包括线性项和偏差项。  </p>
<h3 id="FFM"><a href="#FFM" class="headerlink" title="FFM"></a>FFM</h3><p>FFM的想法源于PITF，该推荐用于具有个性化标签的推荐系统。在PITF中，他们假设三个可用的字段，包括用户，项目和标签，并在单独的潜在空间中分解（用户，项目），（用户，标签）和（项目，标签）。FFM是使用此信息的FM的变体。为了解释FFM如何工作，我们考虑以下新示例：  </p>
<h1 id="-2"><a href="#-2" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-12-11%20%E4%B8%8B%E5%8D%883.13.35.png" alt=""></h1><p>回想一下FM，φFM（w，x）是</p>
<h1 id="-3"><a href="#-3" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-12-11%20%E4%B8%8B%E5%8D%883.13.45.png" alt=""></h1><p>在FM中，每个特征只有一个隐向量来学习具有任何其他特征的潜在影响。 以ESPN为例，ESPN用于学习Nike（w ESPN·w Nike）和Male（w ESPN·w Male）的潜在影响。 然而，由于Nike和Male属于不同的领域，（EPSN，Nike）和（EPSN，Male）的潜在影响可能不同。<br>在FFM中，每个特征都有几个隐向量。根据其他功能的领域，其中一个用于内积。 在我们的例子中，φFFM（w，x）是  </p>
<p>w ESPN,A · w Nike,P + w ESPN,G · w Male,P + w Nike,G · w Male,A  </p>
<p>我们看到要学习（ESPN，NIKE），ESPN，A的潜在效果，因为Nike属于现场广告，因为ESPN属于现场发行人，所以使用了Nike，P. 再次，为了学习（EPSN，男性），ESPN的潜在影响，使用G是因为男性属于性别性别，并且因为ESPN属于现场出版商而使用了男性，P。在数学上，FFM：</p>
<h1 id="-4"><a href="#-4" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-12-11%20%E4%B8%8B%E5%8D%883.18.18.png" alt=""></h1><p>其中f 1和f 2分别是j 1和j 2的字段。 如果f是字段数，那么FFM的变量数是nfk，而计算（4）的复杂度是O（n_mean^2 k）。 值得注意的是，在FFM中，因为每个潜在的向量只需要通过一个特定的领域来学习效果，通常     k_FFM &lt;&lt; k_FM .</p>
<h3 id="解决优化问题"><a href="#解决优化问题" class="headerlink" title="解决优化问题"></a>解决优化问题</h3><p>除了用φFFM（w，x）代替φLM（w，x）之外，优化问题与（1）相同。最近，已经提出了一些自适应学习速率调度表，以促进SGD的训练过程。 我们使用AdaGrad ，因为已经证明了它对矩阵因子化的有效性，这是FFM的一个特例。训练过程如下所示：</p>
<h1 id="-5"><a href="#-5" class="headerlink" title=""></a><img src="http://pudjsgimg.bkt.clouddn.com/2019-07-10-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-12-11%20%E4%B8%8B%E5%8D%886.14.46.png" alt=""></h1><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在本文中，讨论了FFM的有效实现。我们证明，对于某些类型的数据集，FFM在logloss方面优于三个着名的LM，Poly2和FM模型，但训练时间较长。</p>
<h2 id="个人体会"><a href="#个人体会" class="headerlink" title="个人体会"></a>个人体会</h2><p>在FM模型中，每一个特征会对应一个隐变量，但在FFM模型中，认为应该将特征分为多个field，每个特征对应每个field分别有一个隐变量。举个例子，我们的样本有3种类型的字段：publisher, advertiser, gender，分别可以代表媒体，广告主或者是具体的商品，性别。其中publisher有5种数据，advertiser有10种数据，gender有男女2种，经过one-hot编码以后，每个样本有17个特征，其中只有3个特征非空。如果使用FM模型，则17个特征，每个特征对应一个隐变量。如果使用FFM模型，则17个特征，每个特征对应3个隐变量，即每个类型对应一个隐变量，具体而言，就是对应publisher, advertiser, gender三个field各有一个隐变量。  </p>
<p>在FFM的训练过程中，有许多细节要注意。  </p>
<p>第一，样本归一化。FFM默认是进行样本数据的归一化，否则很容易造成数据inf溢出，进而引起梯度计算的nan错误。因此，样本层面的数据是推荐进行归一化的。 </p>
<p>第二，特征归一化。CTR/CVR模型采用了多种类型的源特征，包括数值型和categorical类型等。但是，categorical类编码后的特征取值只有0或1，较大的数值型特征会造成样本归一化后categorical类生成特征的值非常小，没有区分性。例如，一条用户-商品记录，用户为“男”性，商品的销量是5000个（假设其它特征的值为零），那么归一化后特征“sex=male”（性别为男）的值略小于0.0002，而“volume”（销量）的值近似为1。特征“sex=male”在这个样本中的作用几乎可以忽略不计，这是相当不合理的。因此，将源数值型特征的值归一化到 [0,1][0,1] 是非常必要的。  </p>
<p>第三，省略零值特征。从FFM模型的表达式可以看出，零值特征对模型完全没有贡献。包含零值特征的一次项和组合项均为零，对于训练模型参数或者目标值预估是没有作用的。因此，可以省去零值特征，提高FFM模型训练和预测的速度，这也是稀疏样本采用FFM的显著优势。</p>
<p>参考链接：<br><a href="https://blog.csdn.net/jediael_lu/article/details/77772565#21-%E8%83%8C%E6%99%AF%E5%8F%8A%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86" target="_blank" rel="noopener">https://blog.csdn.net/jediael_lu/article/details/77772565#21-%E8%83%8C%E6%99%AF%E5%8F%8A%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Witton Zhou</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  

  

  

</body>
</html>
