<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="f95QClvWkWuCmeDiyOR8qqZeVKyONBqEfTEnHD_rpa4" />








  <meta name="baidu-site-verification" content="1l6QQqPWSs" />







  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/wittonzhou-medium.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/wittonzhou-small.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="WittonZhou">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="WittonZhou">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="WittonZhou">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>WittonZhou</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">WittonZhou</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">业精于勤荒于嬉，行成于思毁于随</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/浅析Xgboost原理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Witton Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WittonZhou">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/浅析Xgboost原理/" itemprop="url">浅析Xgboost原理</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-15T22:31:16+08:00">
                2018-11-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="浅析Xgboost原理"><a href="#浅析Xgboost原理" class="headerlink" title="浅析Xgboost原理"></a>浅析Xgboost原理</h2><h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ol>
<li>泰勒公式</li>
<li>最优化方法</li>
<li>从参数空间到函数空间</li>
<li>详解Xgboost</li>
<li>LightGBM</li>
</ol>
<hr>
<p>泰勒公式：</p>
<h1 id=""><a href="#" class="headerlink" title=""></a><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fx9ptfs9ucj30um0n0aed.jpg" alt=""></h1><p>梯度下降法：</p>
<h1 id="-1"><a href="#-1" class="headerlink" title=""></a><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fx9xboi1ysj30sp0lcwio.jpg" alt=""></h1><p>牛顿法：</p>
<h1 id="-2"><a href="#-2" class="headerlink" title=""></a><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fx9xeiixnhj30sg0l8n0g.jpg" alt=""></h1><h3 id="从参数空间到函数空间"><a href="#从参数空间到函数空间" class="headerlink" title="从参数空间到函数空间"></a>从参数空间到函数空间</h3><p>GBDT在函数空间中利用梯度下降法进行优化<br>Xgboost在函数空间中用牛顿法进行优化  </p>
<h1 id="-3"><a href="#-3" class="headerlink" title=""></a><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fx9xjetx2jj30sn0lgjuw.jpg" alt=""></h1><p>Xgboost的正则项：  </p>
<h1 id="-4"><a href="#-4" class="headerlink" title=""></a><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fx9yv4msvjj30s90lk78t.jpg" alt=""></h1><p>误差函数的二阶泰勒展开：  </p>
<h1 id="-5"><a href="#-5" class="headerlink" title=""></a><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fx9yzlike8j30t10lqad5.jpg" alt=""></h1><h1 id="-6"><a href="#-6" class="headerlink" title=""></a><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fx9z08gz61j30sy0lfgo4.jpg" alt=""></h1><p>Xgboost的打分函数：  </p>
<h1 id="-7"><a href="#-7" class="headerlink" title=""></a><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fx9yxgpquwj30sw0l3n0m.jpg" alt=""></h1><p>树节点分裂算法–精确算法，遍历所有特征的所有可能的分割点，计算gain值，选取值最大的（feature，value）去分割</p>
<h1 id="-8"><a href="#-8" class="headerlink" title=""></a><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fx9xqat898j30su0ldgst.jpg" alt=""></h1><p>树节点分裂方法–近似算法，对于每个特征，只考察分位点，减少计算复杂度</p>
<h1 id="-9"><a href="#-9" class="headerlink" title=""></a><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fx9xssxkooj30sl0lan4w.jpg" alt=""></h1><p>树节点分裂方法–近似算法举例：三分位数  </p>
<p>实际上Xgboost不是简单地按照样本个数进行分为，而是以二阶导数值作为权重。</p>
<h3 id="Xgboost的其他特性"><a href="#Xgboost的其他特性" class="headerlink" title="Xgboost的其他特性"></a>Xgboost的其他特性</h3><ol>
<li>行抽样（row sample）</li>
<li>列抽样（column sample），借鉴了随机森林</li>
<li>Shrikage（缩减），即学习速率。将学习速率调小，迭代次数增多，有正则化作用</li>
<li>支持自定义损失函数（需二阶可导）</li>
</ol>
<p>Column Block 和 Cache Aware Access：  </p>
<h1 id="-10"><a href="#-10" class="headerlink" title=""></a><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fx9y1tzmfoj30sr0kv0z5.jpg" alt=""></h1><h3 id="LightGBM的改进"><a href="#LightGBM的改进" class="headerlink" title="LightGBM的改进"></a>LightGBM的改进</h3><ol>
<li><p>直方图算法：<br>把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。  </p>
<p> 这样减少了内存占用；也减少了split finding时计算增益的计算量。</p>
</li>
<li><p>直方图差加速<br> 一个叶子的直方图可以由它的父亲节点的直方图与它兄弟节点的 直方图做差得到，提升一倍速度。</p>
</li>
<li><p>建树过程的两种方法<br> Level-wise tree growth：同一层所有节点都做分裂，最后剪枝。代表有Xgboost<br> Leaf-wise tree growth：选取具有最大增益节点分裂，容易过拟合，可以通过max_depth限制。代表有LightGBM</p>
</li>
<li><p>并行优化<br> LightGBM的特征并行：<br> 每个worker保存所有数据集<br>（1）每个worker在其特征子集上寻找最佳切分点<br>（2）worker之间互相通信，找到全局最佳切分点<br>（3）每个worker根据全局最佳切分点进行节点分裂  </p>
<p> 优点：避免广播instance indices，减小网络通信量<br> 缺点：split finding计算复杂度没有减小；且当数据量比较大时，单个worker存储所有数据代价高  </p>
</li>
</ol>
<p>传统的数据并行<br>（1）水平切分数据，每个worker只有部分数据<br>（2）每个worker根据本地数据统计局部直方图<br>（3）合并所有局部直方图得到全局直方图<br>（4）根据全局直方图进行节点分裂  </p>
<p>缺点：网络通信代价巨大  </p>
<p>LightGBM的数据并行<br>（1）不同的worker合并不同特征的局部直方图<br>（2）采用直方图做差算法，只需要通信一个节点的直方图</p>
<p>LightGBM其他地方的改进：  </p>
<h1 id="-11"><a href="#-11" class="headerlink" title=""></a><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fx9ylg5zfxj30sn0j2jwd.jpg" alt=""></h1><h3 id="Xgboost和GBDT的区别"><a href="#Xgboost和GBDT的区别" class="headerlink" title="Xgboost和GBDT的区别"></a>Xgboost和GBDT的区别</h3><ol>
<li><p>传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。</p>
</li>
<li><p>传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。</p>
</li>
<li><p>xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。</p>
</li>
<li><p>Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）</p>
</li>
<li><p>列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。</p>
</li>
<li><p>对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。</p>
</li>
<li><p>xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</p>
</li>
<li><p>可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。</p>
</li>
</ol>
<h3 id="Xgboost使用经验总结"><a href="#Xgboost使用经验总结" class="headerlink" title="Xgboost使用经验总结"></a>Xgboost使用经验总结</h3><ol>
<li><p>多类别分类时，类别需要从0开始编码</p>
</li>
<li><p>Watchlist不会影响模型训练。</p>
</li>
<li><p>类别特征必须编码，因为xgboost把特征默认都当成数值型的</p>
</li>
<li><p>训练的时候，为了结果可复现，记得设置随机数种子。</p>
</li>
<li><p>XGBoost的特征重要性是如何得到的？某个特征的重要性（feature score），等于它被选中为树节点分裂特征的次数的和，比如特征A在第一次迭代中（即第一棵树）被选中了1次去分裂树节点，在第二次迭代被选中2次…..那么最终特征A的feature score就是 1+2+….</p>
</li>
</ol>
<p>参考链接：<br><a href="http://wepon.me/2016/05/07/XGBoost%E6%B5%85%E5%85%A5%E6%B5%85%E5%87%BA/" target="_blank" rel="noopener">http://wepon.me/2016/05/07/XGBoost%E6%B5%85%E5%85%A5%E6%B5%85%E5%87%BA/</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/GBDT-Gradient-Boosting-Decision-Tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Witton Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WittonZhou">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/GBDT-Gradient-Boosting-Decision-Tree/" itemprop="url">GBDT(Gradient Boosting Decision Tree)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-15T18:16:02+08:00">
                2018-11-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h2><p>优点：效果好；既可以用于分类也可以用于回归；可以筛选特征；可以灵活处理各种类型的数据，包括连续值和离散值。<br>缺点：由于弱学习器之间存在依赖关系，难以并行训练数据。</p>
<p>GBDT又叫MART（Multiple Additive Regression Tree），是一种迭代的决策树算法。GBDT几乎可用于所有回归问题（线性/非线性），相对logistic regression仅能用于线性回归，GBDT的适用面非常广。亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。GBDT无论用于分类还是回归一直都是使用的CART回归树。<br>在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是ft−1(x), 损失函数是L(y,ft−1(x)), 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器ht(x)，让本轮的损失函数L(y,ft(x)=L(y,ft−1(x)+ht(x))最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。  </p>
<h3 id="1-Regression-Decision-Tree：回归树"><a href="#1-Regression-Decision-Tree：回归树" class="headerlink" title="1.Regression Decision Tree：回归树"></a>1.Regression Decision Tree：回归树</h3><p>回归树总体流程类似于分类树，区别在于，回归树的每一个节点都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化平方误差（最小化均方差）。也就是被预测出错的人数越多，错的越离谱，平方误差就越大，通过最小化平方误差能够找到最可靠的分枝依据。分枝直到每个叶子节点上人的年龄都唯一或者达到预设的终止条件(如叶子个数上限)，若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。  </p>
<p>回归树算法如下图（截图来自《统计学习方法》5.5.1 CART生成）： </p>
<h1 id=""><a href="#" class="headerlink" title=""></a><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fx8x32ajhyj311n0u0tr2.jpg" alt=""></h1><h3 id="2-Boosting-Decision-Tree：提升树算法"><a href="#2-Boosting-Decision-Tree：提升树算法" class="headerlink" title="2.Boosting Decision Tree：提升树算法"></a>2.Boosting Decision Tree：提升树算法</h3><p>提升树是迭代多棵回归树来共同决策。当采用平方误差损失函数时，每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树，残差的意义如公式：残差 = 真实值 - 预测值 。提升树即是整个迭代过程生成的回归树的累加。  </p>
<p>示例如下：</p>
<h1 id="-1"><a href="#-1" class="headerlink" title=""></a><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fx8x4wjl52j30zo0u0h51.jpg" alt=""></h1><p>提升树算法：  </p>
<h1 id="-2"><a href="#-2" class="headerlink" title=""></a><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fx8x63w79lj30bu08b3zc.jpg" alt=""></h1><h3 id="3-GBDT：梯度提升决策树"><a href="#3-GBDT：梯度提升决策树" class="headerlink" title="3.GBDT：梯度提升决策树"></a>3.GBDT：梯度提升决策树</h3><p>利用最速下降的近似方法，即利用损失函数的负梯度在当前模型的值，作为回归问题中提升树算法的残差的近似值，拟合一个回归树。<br>算法如下：  </p>
<h1 id="-3"><a href="#-3" class="headerlink" title=""></a><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fx8x7m8fguj30i90eimyz.jpg" alt=""></h1><p>I(x∈Rjm)其实就是一个很简单的函数，如果x∈Rtj，那么I(x∈Rjm)=1，否则等于0。</p>
<p>算法步骤解释：  </p>
<ol>
<li>初始化弱学习器f0，估计使损失函数极小化的常数值，它是只有一个根节点的树，即ganma是一个常数值。  </li>
<li>对迭代轮数m=1~M有<br>（a）计算损失函数的负梯度在当前模型的值，将它作为残差的估计<br>（b）估计回归树叶节点区域，以拟合残差的近似值<br>（c）利用线性搜索估计叶节点区域的值，使损失函数极小化<br>（d）更新回归树（更新强学习器）  </li>
<li>得到输出的最终模型 f(x)（得到强学习器f（x）的表达式）  </li>
</ol>
<p>注：ganma是GBDT中每颗决策树拟合叶子节点最好的输出值，在回归GBDT初始化的时候，ganma一般取值为所有y的平均值。  </p>
<p>当我们迭代到第t个弱分类器的时候，我们把损失函数进行一阶泰勒展开，出现的f_t-1（x）+ △x 的形式，这个△x 就是我们这一轮弱分类器想要输出的值，一阶泰勒展开后面有一项是导数和△x的乘积。我们当然想让这一轮的损失函数小于上一轮的，那我们就让△x等于负导数 ，这样一个值的平方加上负号，一定是一个负值，这样就能保证这一轮的损失函数小于上一轮的。</p>
<p>GBDT回归算法：</p>
<h1 id="-4"><a href="#-4" class="headerlink" title=""></a><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fx9299dbwyj30hu0gtwgd.jpg" alt=""></h1><p>GBDT二分类算法：</p>
<h1 id="-5"><a href="#-5" class="headerlink" title=""></a><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fx92ahgwioj30ft0a8wft.jpg" alt=""></h1><h3 id="4-重要参数的意义和设置"><a href="#4-重要参数的意义和设置" class="headerlink" title="4.重要参数的意义和设置"></a>4.重要参数的意义和设置</h3><p>推荐GBDT树的深度：6（横向比较：DecisionTree/RandomForest需要把树的深度调到15或更高）<br>以下摘自知乎上的一个问答（详见参考文献8），问题和回复都很好的阐述了这个参数设置的数学原理。<br>【问】xgboost/gbdt在调参时为什么树的深度很少就能达到很高的精度？  </p>
<p>用xgboost/gbdt在在调参的时候把树的最大深度调成6就有很高的精度了。但是用DecisionTree/RandomForest的时候需要把树的深度调到15或更高。用RandomForest所需要的树的深度和DecisionTree一样我能理解，因为它是用bagging的方法把DecisionTree组合在一起，相当于做了多次DecisionTree一样。但是xgboost/gbdt仅仅用梯度上升法就能用6个节点的深度达到很高的预测精度，使我惊讶到怀疑它是黑科技了。请问下xgboost/gbdt是怎么做到的？它的节点和一般的DecisionTree不同吗？  </p>
<p>【答】<br>  这是一个非常好的问题，题主对各算法的学习非常细致透彻，问的问题也关系到这两个算法的本质。这个问题其实并不是一个很简单的问题，我尝试用我浅薄的机器学习知识对这个问题进行回答。  </p>
<p>一句话的解释，来自周志华老师的机器学习教科书（机器学习-周志华）：Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成；Bagging主要关注降低方差，因此它在不剪枝的决策树、神经网络等学习器上效用更为明显。</p>
<p>随机森林(random forest)和GBDT都是属于集成学习（ensemble learning)的范畴。集成学习下有两个重要的策略Bagging和Boosting。  </p>
<p>Bagging算法是这样做的：每个分类器都随机从原样本中做有放回的采样，然后分别在这些采样后的样本上训练分类器，然后再把这些分类器组合起来。简单的多数投票一般就可以。其代表算法是随机森林。Boosting的意思是这样，他通过迭代地训练一系列的分类器，每个分类器采用的样本分布都和上一轮的学习结果有关。其代表算法是AdaBoost, GBDT。  </p>
<p>其实就机器学习算法来说，其泛化误差可以分解为两部分，偏差（bias)和方差(variance)。这个可由下图的式子导出（这里用到了概率论公式D(X)=E(X2)-[E(X)]2）。偏差指的是算法的期望预测与真实预测之间的偏差程度，反应了模型本身的拟合能力；方差度量了同等大小的训练集的变动导致学习性能的变化，刻画了数据扰动所导致的影响。这个有点儿绕，不过你一定知道过拟合。  </p>
<p>如下图所示，当模型越复杂时，拟合的程度就越高，模型的训练偏差就越小。但此时如果换一组数据可能模型的变化就会很大，即模型的方差很大。所以模型过于复杂的时候会导致过拟合。  </p>
<p>当模型越简单时，即使我们再换一组数据，最后得出的学习器和之前的学习器的差别就不那么大，模型的方差很小。还是因为模型简单，所以偏差会很大。  </p>
<p>模型复杂度与偏差方差的关系图：  </p>
<h1 id="-6"><a href="#-6" class="headerlink" title=""></a><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fx8xddvtfxj30cn07tgmc.jpg" alt=""></h1><p>也就是说，当我们训练一个模型时，偏差和方差都得照顾到，漏掉一个都不行。  </p>
<p>对于Bagging算法来说，由于我们会并行地训练很多不同的分类器的目的就是降低这个方差(variance) ,因为采用了相互独立的基分类器多了以后，h的值自然就会靠近.所以对于每个基分类器来说，目标就是如何降低这个偏差（bias),所以我们会采用深度很深甚至不剪枝的决策树。  </p>
<p>对于Boosting来说，每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差（bias）,所以对于每个基分类器来说，问题就在于如何选择variance更小的分类器，即更简单的分类器，所以我们选择了深度很浅的决策树。</p>
<h3 id="5-Shrinkage"><a href="#5-Shrinkage" class="headerlink" title="5.Shrinkage"></a>5.Shrinkage</h3><p>Shrinkage（缩减）的思想认为，每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它不完全信任每一个棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。  </p>
<p>没用Shrinkage时：（yi表示第i棵树上y的预测值， y(1~i)表示前i棵树y的综合预测值）<br>y(i+1) = 残差(y1~yi)， 其中： 残差(y1~yi) =  y真实值 - y(1 ~ i)<br>y(1 ~ i) = SUM(y1, …, yi)  </p>
<p>Shrinkage不改变第一个方程，只把第二个方程改为：<br>y(1 ~ i) = y(1 ~ i-1) + step * yi</p>
<p>即Shrinkage仍然以残差作为学习目标，但对于残差学习出来的结果，只累加一小部分（step*残差）逐步逼近目标，step一般都比较小，如0.01~0.001（注意该step非gradient的step），导致各个树的残差是渐变的而不是陡变的。直觉上这也很好理解，不像直接用残差一步修复误差，而是只修复一点点，其实就是把大步切成了很多小步。本质上，Shrinkage为每棵树设置了一个weight，累加时要乘以这个weight，但和Gradient并没有关系。</p>
<h3 id="6-总结"><a href="#6-总结" class="headerlink" title="6.总结"></a>6.总结</h3><p>GBDT主要是通过损失函数的负梯度来拟合本轮损失的近似值（也就是本轮残差的近似值），进而拟合一个CART回归树。根据第t棵回归树，得到对应的叶节点区域。然后根据叶子区域，计算得到最佳拟合值，然后根据更新公式来更新学习器。<br>此外，可以看出Boosting算法是一种加法模型。</p>
<p>参考链接：<br><a href="https://www.jianshu.com/p/005a4e6ac775" target="_blank" rel="noopener">https://www.jianshu.com/p/005a4e6ac775</a><br><a href="https://www.cnblogs.com/pinard/p/6140514.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6140514.html</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/读《解析卷积神经网络》/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Witton Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WittonZhou">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/读《解析卷积神经网络》/" itemprop="url">读《解析卷积神经网络》</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-13T15:33:01+08:00">
                2018-11-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="《解析卷积神经网络–深度学习实践手册》-作者魏秀参"><a href="#《解析卷积神经网络–深度学习实践手册》-作者魏秀参" class="headerlink" title="《解析卷积神经网络–深度学习实践手册》 作者魏秀参"></a>《解析卷积神经网络–深度学习实践手册》 作者魏秀参</h2><hr>
<h3 id="1-CNN基本部件"><a href="#1-CNN基本部件" class="headerlink" title="1.CNN基本部件"></a>1.CNN基本部件</h3><p>1.1 深度学习的“端到端”学习方式  </p>
<p>深度学习的一个重要思想“端到端”的学习方式，属于表示学习的一种。其他机器学习算法，如特征选择算法、分类器算法、集成学习算法等，均假设样本特征表示是给定的，并在此基础上设计具体的机器学习算法。在深度学习时代之前，样本表示基本都使用人工特征，但是人工特征的优劣往往很大程度决定了最终的任务精度。而在深度学习普及之后，人工特征已逐渐被表示学习根据任务自动需求“学到”的特征表示所取代。  </p>
<p>过去解决一个人工智能问题（以图像识别为例）往往通过分 治法将其分解为预处理、特征提取与选择、分类器设计等若干步骤。子问题上的最优解并不意味着就能得到全局问题的最后解。深度学习为我们提供了另一种学习方式，即“端到端”学习方式，整个学习流程并不进行人为的子问题划分，而是完全交给深度学习模型直接学习从原始输入到期望输出的映射。  </p>
<h1 id=""><a href="#" class="headerlink" title=""></a><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fx6t3lctu4j30ib08i75g.jpg" alt=""></h1><p>1.2 卷积层<br>卷积层的“权值共享”特征<br>卷积操作的两个重要的超参数：卷积核大小（filter size）和卷积步长（stride）。  </p>
<p>1.3 池化（pooling）层<br>文中翻译为汇合层。<br>通常使用的汇合操作为平均值汇合和最大值汇合，还有随机汇合介于二者之间。<br>汇合操作实际上就是一种“降采样”操作，另一方面，汇合也看成是一个用p-范数作为非线性映射的“卷积”操作，当p趋近正无穷时就是最常见的最大值汇合。<br>汇合层的引入是仿照人的视觉系统对视觉输入对象进行降维（降采样）和抽象。汇合层有以下三种功效：<br>1.特征不变性。汇合操作时模型更关注是否存在某些特征而不是特征具体的位置，使特征学习包含某种程度自由度，能容忍一些特征微小的位移。<br>2.特征降维。汇合相当于空间范围内做了维度约减，从而使模型可以抽取更广范围的特征。同时减小了下一层输入大小，进而减少计算量和参数个数。<br>3.在一定程度上防止过拟合，更方便优化。  </p>
<p>汇合操作并不是CNN必须的元件或操作。  </p>
<p>1.4 激活函数<br>激活函数的引入为的是增加整个网络的表达能力（即非线性）。<br>Sigmoid型函数将输出响应的值域被压缩到[0,1]之间，0对应了生物神经元的“抑制状态”，1则对应了“兴奋状态”。Sigmoid有一个严重的问题，即梯度的“饱和效应”。在输出值接近1时，梯度接近0，这会导致在误差反向传播过程中导数处于该区域的误差将很难甚至根本无法传递到前层，进而导致整个网络无法训练。<br>ReLU函数是目前深度卷积神经网络中最为常用的激活函数之一。ReLU函数的梯度在x&gt;=0时为1，反之为0。对x&gt;=0部分完全消除了Sigmoid型函数的梯度饱和效应。ReLU函数还有助于随机梯度下降方法收敛，收敛速度约快6倍左右。  </p>
<p>1.5 全连接层<br>全连接层起到将学到的特征表示映射到样本的标记空间的作用。  </p>
<p>1.6 目标函数<br>目标函数的作用是用来衡量该预测值与真实样本标记之间的误差。  </p>
<h3 id="2-卷积神经网络经典结构"><a href="#2-卷积神经网络经典结构" class="headerlink" title="2.卷积神经网络经典结构"></a>2.卷积神经网络经典结构</h3><ol>
<li><p>感受野<br>随着网络深度的加深，后层神经元在第一层输入层的感受野会随之增大。也就是说，小卷积核通过多层叠加可取得与大卷积核同等规模的感受野。<br>小卷积核的优势：<br>（1）由于小卷积核需要多层叠加，加深了网络深度进而增强了网络容量<br>（2）增强网络容量的同时减少了参数个数  </p>
</li>
<li><p>残差网络模型<br>神经网络的深度和宽度是表征网络复杂度的两个核心因素，不过深度相比宽度在增加网络的复杂度方面更有效。然而，随着深度的增加，训练会变得愈加困难。这主要因为在基于随机梯度下降的网络训练过程中，误差信号的多层反向传播非常容易引发梯度“弥散”或者“爆炸”。一些特殊的权重初始化策略和批规范化（batch normalization）可以使这个问题得到极大的改善，但是效果有限。<br>当深度网络收敛时，另外的问题又随之而来：随着继续增加网络的深度，训练数据的训练误差没有降低反而升高。残差网络很好的解决了这个问题。  </p>
</li>
</ol>
<h3 id="3-卷积神经网络的压缩"><a href="#3-卷积神经网络的压缩" class="headerlink" title="3.卷积神经网络的压缩"></a>3.卷积神经网络的压缩</h3><p>按照压缩过程对网络结构的破坏程度，我们将模型压缩技术分为“前端压缩”与“后端压缩”两部分。所谓“前端压缩”，是指不改变原网络结构的压缩技术，主要包括知识蒸馏、紧凑的模型结构设计以及滤波器层面的剪枝等；而“后端压缩”则包括低秩近似、未加限制的剪枝、参数量化以及二值网络等，其目标在于尽可能地减少模型大小，因而会对原始网络结构造成极大程度的改造。  </p>
<p>低秩近似、剪枝与参数量化作为常用的三种压缩技术，已经具备了较为明朗的应用前景；其他压缩技术，如二值网络、知识蒸馏等尚处于发展阶段。  </p>
<ol>
<li><p>低秩近似<br> 使用结构化矩阵来进行低秩分解的算法；也可以直接使用矩阵分解来降低权重矩阵的参数。低秩近似算法在中小型网络模型上取得了很不错的效果，但其超参数量与网络层数呈线性变化趋势，随着网络层数的增加与模型复杂度的提升，其搜索空间会急剧增大。  </p>
</li>
<li><p>剪枝与稀疏约束<br> 给定一个预训练好的网络模型，常用的剪枝算法一般都遵从如下的操作流程：<br> （1）衡量神经元的重要程度。衡量其重要程度的方法也是多种多样，从一些基本的启发式算法，到基于梯度的方案，其计算复杂度与最终的效果也是各有千秋。<br> （2）移除掉一部分不重要的神经元。这里可以根据某个阈值来判断神经元是否可以被剪除，也可以按重要程度排序，剪除掉一定比例的神经元。一般而言，后者比前者更加简便，灵活性也更高。<br> （3）对网络进行微调。由于剪枝操作会不可避免地影响网络的精度，为防止对分类性能造成过大的破坏，需要对剪枝后的模型进行微调。<br> （4）返回第一步，进行下一轮剪枝  </p>
</li>
<li><p>参数量化<br> 最简单也是最基本的一种量化算法便是标量量化。该算法的基本思路是，对于每一个权重矩阵，首先将其转化为向量形式。之后对该权重向量的元素进行 k 个簇的聚类，这可借助于经典的k均值聚类算法快速完成。如此一来，只需将 k 个聚类中心（标量）存储在码本之中便可，而原权重矩阵则只负责记录各自聚类中心在码本中的索引。  </p>
</li>
<li><p>二值网络<br> 二值网络可以被视为量化方法的一种极端情况：所有参数的取值只能是±1。正是这种极端的设定，使得二值网络能够获得极大的压缩效益。  </p>
<p> 首先，在普通的神经网络中，一个参数是由单精度浮点数来表示的，参数的二值化能将存储开销降低为原来的1/32。其次，如果中间结果也能二值化的话，那么所有的运算仅靠位操作便可完成。借助于同或门等逻辑门元件便能快速完成所有的计算。而这一优点是其余压缩方法所不能比拟的。深度神经网络的一大诟病就在于其巨大的计算代价，如果能够获得高准确度的二值网络，那么便可摆脱对GPU等高性能计算设备的依赖。  </p>
</li>
<li><p>知识蒸馏<br> 在不改变模型复杂度的情况下，通过增加监督信息的丰富程度，带来性能上的提升。  </p>
</li>
</ol>
<ol start="6">
<li>紧凑的网络结构<br> 直接训练小模型。设计上不容易，依赖经验和技巧。随着参数数量的降低，网络的泛化性不一定能得到保障。 </li>
</ol>
<h3 id="4-数据扩充"><a href="#4-数据扩充" class="headerlink" title="4.数据扩充"></a>4.数据扩充</h3><p>数据扩充是深度模型训练前的必须一步，此操作可扩充训练数据集，增强数据多样性，防止模型过拟合。<br>简单的数据扩充方式有：水平翻转，随机扣取，尺度变换，旋转等<br>对原图或已变换的图像进行色彩抖动。<br>实践中，往往会将上述几种方式叠加使用，如此可将图像数据扩充至原有数据的数倍甚至数十倍。  </p>
<p>特殊的数据扩充方式：  </p>
<ol>
<li>Fancy PCA  </li>
<li>监督式数据扩充  </li>
</ol>
<h3 id="5-数据预处理"><a href="#5-数据预处理" class="headerlink" title="5.数据预处理"></a>5.数据预处理</h3><p>机器学习中， 对于输入特征做归一化预处理操作是常见的步骤。类似的，在图像处理中，图像的每个像素信息同样可以看做一种特征。在实践中，对每个特征减去平均值来中心化数据是非常重要的，这种归一化处理方式被称作“中心式归一化”（mean normalization）。<br>减均值操作的原理是，我们默认自然图像是一类平稳的数据分布（即数据每一个维度的统计都服从相同分布），此时，在每个样本上减去数据的统计平均值（逐样本计算）可以移除共同部分，凸显个体差异。<br>需要注意的是，在实际操作中应首先划分好训练集、验证集和测试集，而该均值仅针对划分后的训练集计算。不可直接在未划分的所有图像上计算均值，如此会违背机器学习基本原理，即“模型训练过程中能且仅能从训练数据中获取信息”。  </p>
<h3 id="6-网络参数初始化"><a href="#6-网络参数初始化" class="headerlink" title="6.网络参数初始化"></a>6.网络参数初始化</h3><p>理想的网络参数初始化使模型训练事半功倍，相反，糟糕的初始化方案不仅会影响网络收敛甚至会导致梯度弥散或爆炸致使训练失败。介绍几种目前实践中常用的几种网络参数初始化方式。  </p>
<ol>
<li><p>全零初始化  </p>
<p> 当网络收敛到稳定状态时，参数（权值）在理想情况下应基本保持正负各半的状态（此时期望为 0）。因此，一种简单且听起来合理的参数初始化做法是，干脆将所有参数都初始化为0，因为这样可使得初始化全零时参数的期望与网络稳定时参数的期望一致为零。  </p>
<p> 不过，细细想来则会发现参数全为0时网络不同神经元的输出必然相同，相同输出则导致梯度更新完全一样，这样便会令更新后的参数仍然保持一样的状态。换句话说，如若参数进行了全零初始化，那么网络神经元将毫无能力对此做出改变，从而无法进行模型训练。</p>
</li>
<li><p>随机初始化  </p>
<p> 将参数随机化自然是打破上述“僵局”的一个有效手段，不过我们仍然希望所有参数期望依旧接近0。遵循这一原则，我们可将参数值随机设定为接近0的一个很小的随机数（有正有负）。在实际应用中，随机参数服从高斯分布或均匀分布都是较有效的初始化方式。还有一个问题，网络输出数据分布的方差会随着输入神经元个数改变。所以会在初始化的同时加上方差大小的规范化。  </p>
</li>
</ol>
<ol start="3">
<li><p>其他初始化方法  </p>
<p> 利用预训练模型—-将预训练模型的参数作为新任务上模型的参数初始化。</p>
</li>
</ol>
<h3 id="7-激活函数"><a href="#7-激活函数" class="headerlink" title="7.激活函数"></a>7.激活函数</h3><p>激活函数又称为“非线性映射函数”。以下将介绍七种当下深度卷积神经网络中常用的激活函数：<br>1.Sigmoid型函数  </p>
<h1 id="-1"><a href="#-1" class="headerlink" title=""></a><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fx7t2guz5aj305n020dfp.jpg" alt=""></h1><p>Sigmoid型函数值域的均值并非为0而是全为正，这样的结果实际上并不符合我们队神经网络内数值的期望（均值）应为0的设想。</p>
<p>2.tanh(x)型函数  </p>
<h1 id="-2"><a href="#-2" class="headerlink" title=""></a><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fx7t5zyx93j305o0180sk.jpg" alt=""></h1><p>tanh(x)型函数是在Sigmoid型函数基础上为解决均值问题提出的激活函数。tanh(x)型函数又称作双曲正切函数，其函数范围是(-1,+1)，输出响应的均值为0。但是tanh(x)型函数仍基于Sigmoid型函数，依然会发生“梯度饱和”现象。  </p>
<p>3.修正线性单元(ReLU)  </p>
<h1 id="-3"><a href="#-3" class="headerlink" title=""></a><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fx7ta4gvxlj307e043t8t.jpg" alt=""></h1><p>ReLU函数是目前深度卷积神经网络中最为常用的激活函数之一。<br>对 x ≥ 0 部分完全消除了Sigmoid型函数的梯度饱和效应。但是也存在缺陷，即在 x &lt; 0 时，梯度便为0。对于小于0的这部分卷积结果响应，它们一旦变为负值将再无法影响网络训练——这种现象被称作“死区”。  </p>
<p>4.Leaky ReLU<br>为了缓解“死区”现象，研究者将ReLU函数中 x &lt; 0 的部分调整为 f(x) = α·x， 其中 α 为 0.01或0.001数量级的较小正数。  </p>
<h1 id="-4"><a href="#-4" class="headerlink" title=""></a><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fx7teiy3kkj309f02mmx5.jpg" alt=""></h1><p>原始的ReLU函数实际上是Leaky ReLU函数的一个特例。不过由于Leaky ReLU中 α 为超参数，合适的值较难设定且较为敏感，因此Leaky ReLU函数在实际使用中的性能并不十分稳定。  </p>
<p>5.参数化ReLU<br>参数化ReLU的提出很好的解决了Leaky ReLU中超参数 α 不易设定的问题：参数化ReLU直接将 α 也作为一个网络中可学习的变量融入模型的整体训练过程。<br>参数化ReLU在带来更大自由度的同时，也增加了网络模型过拟合的风险，在实际使用中需格外注意。  </p>
<p>6.随机化ReLU<br>另一种解决 α 超参设定的方式是将其随机化，这便是随机化ReLU。  </p>
<p>7.指数化线性单元(ELU)  </p>
<h1 id="-5"><a href="#-5" class="headerlink" title=""></a><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fx7tx5yqr0j30af02q3yj.jpg" alt=""></h1><p>ELU具备ReLU函数的优点， 同时ELU也解决了ReLU函数自身的“死区”问题。不过，ELU函数中的指数操作稍稍增大了计算量。实际使用中，ELU中的超参数 λ 一般设置为1。  </p>
<h3 id="8-目标函数"><a href="#8-目标函数" class="headerlink" title="8.目标函数"></a>8.目标函数</h3><ol>
<li><p>交叉熵损失函数<br> 又称为Softmax损失函数，是目前卷积神经网络中最常用的分类目标函数，通过指数化变换使网络输出h转换为概率形式。<br> <img src="https://ws4.sinaimg.cn/large/006tNbRwly1fx7vtk2g21j30ey01zwei.jpg" alt="">  </p>
</li>
<li><p>合页损失函数<br> 在SVM中被广泛使用。<br> <img src="https://ws2.sinaimg.cn/large/006tNbRwly1fx7vw3iuosj3092021mx4.jpg" alt=""><br> 一般情况的分类任务中，交叉熵损失函数的分类效果略优于合页损失函数的分类效果。  </p>
</li>
<li><p>坡道损失函数  </p>
</li>
<li>大间隔交叉熵损失函数</li>
<li>中心损失函数  </li>
</ol>
<p>总结：<br>交叉熵损失函数是最为常用的分类目标函数，且效果一般优于合页损失函数；<br>大间隔损失函数和中心损失函数从增大类间距离、减小类内距离的角度不仅要求分类准确，而且还有助提高特征的分辨能力；<br>坡道损失函数是分类问题目标函数中的一类非凸损失函数，由于其良好的抗噪特性，推荐将其用于样本噪声或离群点较多的分类任务。  </p>
<h3 id="9-网络正则化"><a href="#9-网络正则化" class="headerlink" title="9.网络正则化"></a>9.网络正则化</h3><p>1.L2正则化<br>L2正则化方式在深度学习中有个常用的叫法是“权重衰减”， 另外L2正则化在机器学习中还被称作“岭回归”或Tikhonov正则化。  </p>
<p>2.L1正则化  </p>
<p>3.最大范数约束<br>最大范数约束是通过向参数量级的范数设置上限对网络进行正则化的手段。  </p>
<p>4.dropout(随机失活)<br>随机失活的提出正是一定程度上缓解了神经元之间复杂的协同适应，降低了神经元间依赖，避免了网络过拟合的发生。其原理非常简单：对于某层的每个神经元，在训练阶段均以概率 p 随机将该神经元权重置0（故被称作“随机失活”），测试阶段所有神经元均呈激活态，但其权重需乘 (1 − p) 以保证训练和测试阶段各自权重拥有相同的期望。  </p>
<p>由于失活的神经元无法参与到网络训练，因此每次训练（前向操作和反向操作）时相当于面对一个全新网络。以含两层网络、各层有三个神经元的简单神经网络为例，若每层随机失活一个神经元，该网络共可产生9种子网络。根据上述随机失活原理，训练阶段相当于共训练了9个子网络，测试阶段则相当于9个子网络的平均集成。  </p>
<h1 id="-6"><a href="#-6" class="headerlink" title=""></a><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fx7u98eam8j30h00bj406.jpg" alt=""></h1><p>5.验证集的使用<br>借助验证集对网络训练“早停”是一种有效的防止网络过拟合方法。</p>
<h3 id="10-超参数设定和网络训练"><a href="#10-超参数设定和网络训练" class="headerlink" title="10.超参数设定和网络训练"></a>10.超参数设定和网络训练</h3><ol>
<li><p>输入数据像素大小<br> 统一将图像压缩到2^n大小，便于GPU设备并行。  </p>
</li>
<li><p>卷积层参数的设定<br> 包括卷积核大小、卷积操作的步长和卷积核个数。<br> 实践中推荐使用3 <em> 3以及 5 </em> 5，其对应卷积操作步长为1。<br> 为了硬件字节级存储管理的方便，卷积核个数通常设置为2 的次幂，如64，128，512等。  </p>
</li>
<li>汇合层参数的设定<br> 常用的参数设定为2 <em> 2、汇合步长为2，起到“降采样”的作用。为了不丢弃过多输入相应而损失网络性能，汇合操作极少使用超过3 </em> 3大小的汇合操作。  </li>
</ol>
<hr>
<p>训练技巧  </p>
<ol>
<li>训练数据随机打乱<br> 在训练卷积神经网络时，尽管训练数据固定，但由于采用了随机批处理（mini-batch）的训练机制，因此我们可在模型每轮（epoch）训练进行前将训练数据集随机打乱（shuffle），确保模型不同轮数相同批次“看到”的数据是不同的。  </li>
<li>学习率的设定<br> （1）模型训练开始时的初始学习率不宜过大，以0.01和0.001为宜<br> （2）模型训练过程中，学习率应随轮数增加而减缓。也可以分为，轮数减缓、指数减缓、分数减缓。  </li>
<li><p>批规范操作（BN）<br> 在模型每次随机梯度下降训练时，通过mini-batch来对相应的网络响应做规范化操作，使得结果（输出信号各个维度）的均值为0，方差为1。算法如下：<br>  <img src="https://ws4.sinaimg.cn/large/006tNbRwly1fx7v7i4ywgj30ix08uwg0.jpg" alt="">  </p>
<p> BN奏效的原因：在统计机器学习中的一个经典假设是“源空间和目标空间的数据分布（是一致的”。如果不一致，那么就出现了新的机器学习问题，如迁移学习等。通过BN来规范化某些层或所有层的输入，从而可以固定每层输入信号的均值与方差。这样一来，即使网络模型较深层的响应或梯度很小，也可通过BM的规范化作用将其的尺度变大，以此便可解决深层网络训练很可能带来的“梯度弥散” 问题。  </p>
</li>
<li>网络模型优化算法选择<br> （1）随机梯度下降法。经典的随机梯度下降是最常见的神经网络优化方法，收敛效果较稳定，不过收敛速度过慢。<br> （2）基于动量的随机梯度下降法。<br> （3）Nesterov型动量随机下降法。<br> （4）Adagrad法。<br> （5）Adadelta法。<br> （6）RMSProp法。<br> （7）Adam法。  </li>
<li>微调神经网络<br> 微调预训练模型简单来说，就是用目标任务数据在原先预训练模型上继续进行训练过程。微调预训练模型时，需注意学习率调整和原始数据与目标数据的关系。另外，还可使用“多目标学习框架”对预训练模型进行微调。  </li>
</ol>
<h3 id="后面的几章涉及到的都是基础内容，就不做过多介绍。"><a href="#后面的几章涉及到的都是基础内容，就不做过多介绍。" class="headerlink" title="后面的几章涉及到的都是基础内容，就不做过多介绍。"></a>后面的几章涉及到的都是基础内容，就不做过多介绍。</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/23岁，被谈及最多的问题是钱/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Witton Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WittonZhou">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/23岁，被谈及最多的问题是钱/" itemprop="url">23岁，被谈及最多的问题是钱</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-10T10:31:56+08:00">
                2018-11-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id=""><a href="#" class="headerlink" title=""></a><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fx2rhd966uj30ci0kuq3c.jpg" alt=""></h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/读论文-OSIBD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Witton Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WittonZhou">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/读论文-OSIBD/" itemprop="url">读论文:OSIBD</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-04T15:21:06+08:00">
                2018-11-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="《A-Novel-Technique-on-Class-Imbalance-Big-Data-using-Analogous-over-Sampling-Approach》-2017-IJCIR"><a href="#《A-Novel-Technique-on-Class-Imbalance-Big-Data-using-Analogous-over-Sampling-Approach》-2017-IJCIR" class="headerlink" title="《A Novel Technique on Class Imbalance Big Data using Analogous over Sampling Approach》,2017,IJCIR"></a>《A Novel Technique on Class Imbalance Big Data using Analogous over Sampling Approach》,2017,IJCIR</h2><p>大数据中的类别不平衡问题降低了现有的分类器的效果，本文提出了一种新奇的算法，叫Over Sampling on Imbalance Big Data(OSIBD)。结果是，OSIBD算法在处理不平衡类别的问题上效果要比C4.5算法好。</p>
<p>类别不平衡问题影响分类器效果的原因是：只有少数的minority 实例是模型构造过程中可以获取的，所以模型是很难预测unseen minority 实例。</p>
<h3 id="处理类别不平衡的方法有"><a href="#处理类别不平衡的方法有" class="headerlink" title="处理类别不平衡的方法有"></a>处理类别不平衡的方法有</h3><p>1.作用于算法的内部方法<br>调整决策threshold，对少数类产生bias，在学习过程中引入costs来补偿少数群体。</p>
<p>2.作用于数据的外部方法<br>对少数类的过采样和对多数类的欠采样。</p>
<p>3.基于boosting的针对训练集不平衡的综合方法</p>
<h3 id="OSIBD算法"><a href="#OSIBD算法" class="headerlink" title="OSIBD算法"></a>OSIBD算法</h3><p>(1)Preparation of the Majority and Minority subsets<br>准备多数类和少数类的子集</p>
<p>(2)Improve with in class imbalances by removing noisy and borderline<br>instances<br>通过去除噪声和边界实例来改善类别不平衡</p>
<p>为了找到弱实例，其中一种方法是找到最有影响属性或特征，然后移除与该特征相关的噪声或弱属性的范围。</p>
<p>(3)Applying oversampling on the minority subset<br>对少数类进行过采样</p>
<p>通过生成合成实例，副本实例和生成具有现有和合成实例的特征的混合实例，以类似方法对预处理的少数子集进行过采样。</p>
<p>(4)Forming the strong dataset<br>形成足够的数据集</p>
<h3 id="个人心得"><a href="#个人心得" class="headerlink" title="个人心得"></a>个人心得</h3><p>文章中提到的过采样算法比较模糊，没有类似SMOTE的具体的过采样算法。<br>OSIBD的实验对比是和C4.5来进行的，而且是单颗树的。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/读论文-FFM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Witton Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WittonZhou">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/读论文-FFM/" itemprop="url">读论文:FFM</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-18T22:18:45+08:00">
                2018-10-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="《Field-aware-Factorization-Machines-for-CTR-Prediction》-2016"><a href="#《Field-aware-Factorization-Machines-for-CTR-Prediction》-2016" class="headerlink" title="《Field-aware Factorization Machines for CTR Prediction》,2016"></a>《Field-aware Factorization Machines for CTR Prediction》,2016</h2><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>1.在CTR预估上FFM性能超过了FM</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/读论文-Deep-Learning-based-Recommender-System-A-Survey-and-New/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Witton Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WittonZhou">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/读论文-Deep-Learning-based-Recommender-System-A-Survey-and-New/" itemprop="url">读论文: Deep Learning based Recommender System:A Survey and New</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-24T19:16:43+08:00">
                2018-09-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Deep-Learning-based-Recommender-System-A-Survey-and-New-Perspectives-2017"><a href="#Deep-Learning-based-Recommender-System-A-Survey-and-New-Perspectives-2017" class="headerlink" title="Deep Learning based Recommender System:A Survey and New Perspectives, 2017"></a>Deep Learning based Recommender System:A Survey and New Perspectives, 2017</h1><p>RS作为一种克服信息过载的有效解决方案。<br>与传统推荐模型相比，深度学习提供了对用户需求，项目特征和它们之间历史交互的理解。</p>
<p>文章全面回顾了基于DL的RS研究方法，提出了基于DL的推荐模型的分类法。</p>
<p>DL能够有效地捕获非线性和非平凡的用户-项目关系。</p>
<p>RS三种输出的分类：评分预测，top-n，分类</p>
<p>推荐模型通常分类为三种：协同过滤，基于内容，混合推荐系统</p>
<p>DL中的一些模型（能和RS结合的）：<br>MLP：多层感知器<br>AE：自动编码器<br>CNN：卷积神经网络<br>RNN：循环神经网络<br>DSSM：深层语义相似度模型<br>RBM：受限波兹曼机<br>NADE：神经自回归分布估计<br>GAN：生产对抗网络</p>
<p>文章将深度学习的推荐模型分为两大类：使用单深度学习技术的模型和深度复合模型（涉及两种及更多深度学习技术的RS）</p>
<p>深度学习技术决定了这些推荐模型的优势和应用场景。<br>MLP可以轻松地模拟用户和项目之间的非线性交互；<br>CNN能够从异构数据源（如文本和视觉信息）中提取本地和全局的表示；<br>RNN使推荐系统能够对评级数据的时间动态进行建模，并且对内容信息的顺序进行建模；<br>DSSM能够在用户和项目之间执行语义匹配。<br>。。。</p>
<p>NCF（神经网络协同过滤）</p>
<p>基于自动编码器的RS</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/读论文-Factorization-Machines/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Witton Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WittonZhou">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/读论文-Factorization-Machines/" itemprop="url">读论文: Factorization Machines</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-24T19:04:35+08:00">
                2018-09-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="《Factorization-Machines》-2010"><a href="#《Factorization-Machines》-2010" class="headerlink" title="《Factorization Machines》,2010"></a>《Factorization Machines》,2010</h2><p>模型：</p>
<h1 id=""><a href="#" class="headerlink" title=""></a><img src="https://ws4.sinaimg.cn/large/006tNbRwgy1fvku7ctissj30r60jatbg.jpg" alt=""></h1><p>vi的维度k反映了模型的表达能力。<br>如果k被选择地足够大，则FM可以表达任何交互矩阵W。然而在稀疏的情况下，k通常是小的。因为没有足够的数据来估计复杂的W。限制k的大小，会有更好的泛化性能。</p>
<p>推导：</p>
<h1 id="-1"><a href="#-1" class="headerlink" title=""></a><img src="https://ws1.sinaimg.cn/large/006tNbRwgy1fvku82r8tjj30qk0jm76d.jpg" alt=""></h1><p>可知FM参数训练的复杂度为O(kn)。此外，在稀疏的情况下，只用计算非零元素的sum，FM的计算复杂度会更小。</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>1.FM结合了SVM和因式分解模型二者的优势。<br>2.FM可以处理稀疏的数据，而SVM不能。其中，FM处理稀疏数据主要机制是：因式分解参数可以对任何两个特征直接做组合。而SVM在处理稀疏数据的情况下，不能在复杂核空间中学习到可信赖的参数（超平面）。<br>3.FM可以在线性时间完成计算，不需要像SVM去做对偶转换。</p>
<h2 id="FM的优点"><a href="#FM的优点" class="headerlink" title="FM的优点"></a>FM的优点</h2><p>1.FM允许在SVM失败的非常稀疏的数据下进行参数估计<br>2.FM具有线性复杂度，可以在原始中进行优化，而不依赖于支持向量，如SVM。<br>3.FM是一种可以与任何实值特征向量一起使用的通用预测器。与此相反，其他最先进的分解模型仅适用于非常有限的输入数据。</p>
<h2 id="FM与SVM比较"><a href="#FM与SVM比较" class="headerlink" title="FM与SVM比较"></a>FM与SVM比较</h2><p>1.SVM的密集参数化需要直接观察相互作用，这个在稀疏设置中通常不会给出。即使在稀疏的情况在，也可以很好的估计FM的参数。<br>2.FM可以在原始中直接学习。非线性SVM通常在对偶形式中学习。<br>3.FMs的模型方程独立于训练数据。使用SVM进行预测取决于训练数据的部分（支持向量）。<br>4.FM计算时间与参数是呈线性关系，存储模型的时候也不需要存储训练数据，而SVM需要存存储训练数据（支持向量），SVM在对偶空间中优化。</p>
<h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>回归，二分类，排序</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/常见的SVD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Witton Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WittonZhou">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/常见的SVD/" itemprop="url">常见的SVD</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-16T14:55:18+08:00">
                2018-06-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-Traditional-SVD："><a href="#1-Traditional-SVD：" class="headerlink" title="1.Traditional SVD："></a>1.Traditional SVD：</h2><p>如果想运用SVD分解的话，有一个前提是要求矩阵是稠密的，即矩阵里的元素要非空，否则就不能运用SVD分解。很显然我们的任务还不能用SVD，所以一般的做法是先用均值或者其他统计学方法来填充矩阵，然后再运用SVD分解降维。</p>
<h2 id="2-FunkSVD："><a href="#2-FunkSVD：" class="headerlink" title="2.FunkSVD："></a>2.FunkSVD：</h2><p>不再将矩阵分解为3个矩阵，而是分解为2个低秩的用户项目矩阵。<br>公式：</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fsd1l25eggj304601emwy.jpg" alt=""></p>
<p>带有L2正则项的FunkSVD，公式为：<br><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fsd1nbcmjij30bb01xjrd.jpg" alt=""></p>
<h2 id="3-BiasSVD："><a href="#3-BiasSVD：" class="headerlink" title="3.BiasSVD："></a>3.BiasSVD：</h2><p>在FunkSVD提出来之后，出现了很多变形版本，其中一个相对成功的方法是BiasSVD，顾名思义，即带有偏置项的SVD分解，公式：</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fsd1r1it7kj30eg01hglj.jpg" alt=""></p>
<h2 id="4-SVD-："><a href="#4-SVD-：" class="headerlink" title="4.SVD++："></a>4.SVD++：</h2><p>人们后来又提出了改进的BiasSVD，还是顾名思义，两个加号，我想是一个加了用户项目偏置项，另一个是在它的基础上添加了用户的隐式反馈信息。公式：</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fsd1qnhm8hj30cl02rmx7.jpg" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Witton Zhou</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/wittonzhou" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:wittonzhou@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Witton Zhou</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  

  

  

</body>
</html>
